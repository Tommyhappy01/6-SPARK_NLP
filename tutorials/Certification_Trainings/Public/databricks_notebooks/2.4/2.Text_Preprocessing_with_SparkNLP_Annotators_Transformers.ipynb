{"cells":[{"cell_type":"markdown","source":["![JohnSnowLabs](https://nlp.johnsnowlabs.com/assets/images/logo.png)"],"metadata":{"colab_type":"text","id":"sXatvRX899i0"}},{"cell_type":"markdown","source":["# 2. Text Preprocessing with Spark NLP"],"metadata":{"colab_type":"text","id":"Xj5fx5ir-wMt"}},{"cell_type":"markdown","source":["**Note** Read this article if you want to understand the basic concepts in Spark NLP.\n\nhttps://towardsdatascience.com/introduction-to-spark-nlp-foundations-and-basic-components-part-i-c83b7629ed59"],"metadata":{}},{"cell_type":"markdown","source":["## 1. Annotators and Transformer Concepts"],"metadata":{"colab_type":"text","id":"SS07N80gEtSt"}},{"cell_type":"markdown","source":["In Spark NLP, all Annotators are either Estimators or Transformers as we see in Spark ML. An Estimator in Spark ML is an algorithm which can be fit on a DataFrame to produce a Transformer. E.g., a learning algorithm is an Estimator which trains on a DataFrame and produces a model. A Transformer is an algorithm which can transform one DataFrame into another DataFrame. E.g., an ML model is a Transformer that transforms a DataFrame with features into a DataFrame with predictions.\nIn Spark NLP, there are two types of annotators: AnnotatorApproach and AnnotatorModel\nAnnotatorApproach extends Estimators from Spark ML, which are meant to be trained through fit(), and AnnotatorModel extends Transformers which are meant to transform data frames through transform().\nSome of Spark NLP annotators have a Model suffix and some do not. The model suffix is explicitly stated when the annotator is the result of a training process. Some annotators, such as Tokenizer are transformers but do not contain the suffix Model since they are not trained, annotators. Model annotators have a pre-trained() on its static object, to retrieve the public pre-trained version of a model.\nLong story short, if it trains on a DataFrame and produces a model, it’s an AnnotatorApproach; and if it transforms one DataFrame into another DataFrame through some models, it’s an AnnotatorModel (e.g. WordEmbeddingsModel) and it doesn’t take Model suffix if it doesn’t rely on a pre-trained annotator while transforming a DataFrame (e.g. Tokenizer).\n\nBy convention, there are three possible names:\n\nApproach — Trainable annotator\n\nModel — Trained annotator\n\nnothing — Either a non-trainable annotator with pre-processing step or shorthand for a model\n\nSo for example, Stemmer doesn’t say Approach nor Model, however, it is a Model. On the other hand, Tokenizer doesn’t say Approach nor Model, but it has a TokenizerModel(). Because it is not “training” anything, but it is doing some preprocessing before converting into a Model. When in doubt, please refer to official documentation and API reference. Even though we will do many hands-on practices in the following articles, let us give you a glimpse to let you understand the difference between AnnotatorApproach and AnnotatorModel. As stated above, Tokenizer is an AnnotatorModel. So we need to call fit() and then transform().\n\nNow let’s see how this can be done in Spark NLP using Annotators and Transformers. Assume that we have the following steps that need to be applied one by one on a data frame.\n\nSplit text into sentences\nTokenize\nNormalize\nGet word embeddings\nimage.png\n\nWhat’s actually happening under the hood?\n\nWhen we fit() on the pipeline with Spark data frame (df), its text column is fed into DocumentAssembler() transformer at first and then a new column “document” is created in Document type (AnnotatorType). As we mentioned before, this transformer is basically the initial entry point to Spark NLP for any Spark data frame. Then its document column is fed into SentenceDetector() (AnnotatorApproach) and the text is split into an array of sentences and a new column “sentences” in Document type is created. Then “sentences” column is fed into Tokenizer() (AnnotatorModel) and each sentence is tokenized and a new column “token” in Token type is created. And so on."],"metadata":{"colab_type":"text","id":"g3_ic8K7E0sy"}},{"cell_type":"code","source":["import sparknlp\n\nspark = sparknlp.start()\n\nprint(\"Spark NLP version\", sparknlp.version())\n\nprint(\"Apache Spark version:\", spark.version)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Spark NLP version 2.5.2\nApache Spark version: 2.4.5\n</div>"]}}],"execution_count":6},{"cell_type":"markdown","source":["## Create Spark Dataframe"],"metadata":{}},{"cell_type":"code","source":["text = 'Peter Parker is a nice guy and lives in New York'\n\nspark_df = spark.createDataFrame([[text]]).toDF(\"text\")\n\nspark_df.show(truncate=False)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":123},"colab_type":"code","id":"qj-Q4LzMGCtQ","outputId":"f36c1651-9ac0-434d-fdd7-bbb0c9e2cf4c"},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+------------------------------------------------+\ntext                                            |\n+------------------------------------------------+\nPeter Parker is a nice guy and lives in New York|\n+------------------------------------------------+\n\n</div>"]}}],"execution_count":8},{"cell_type":"code","source":["# if you want to create a spark datafarme from a list of strings\nfrom pyspark.sql.types import StringType\ntext_list = ['Peter Parker is a nice guy and lives in New York.', 'Bruce Wayne is also a nice guy and lives in Gotham City.']\n\nspark.createDataFrame(text_list, StringType()).toDF(\"text\").show(truncate=80)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+--------------------------------------------------------+\n                                                    text|\n+--------------------------------------------------------+\n       Peter Parker is a nice guy and lives in New York.|\nBruce Wayne is also a nice guy and lives in Gotham City.|\n+--------------------------------------------------------+\n\n</div>"]}}],"execution_count":9},{"cell_type":"code","source":["from pyspark.sql import Row\n\nspark.createDataFrame(list(map(lambda x: Row(text=x), text_list))).show(truncate=80)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+--------------------------------------------------------+\n                                                    text|\n+--------------------------------------------------------+\n       Peter Parker is a nice guy and lives in New York.|\nBruce Wayne is also a nice guy and lives in Gotham City.|\n+--------------------------------------------------------+\n\n</div>"]}}],"execution_count":10},{"cell_type":"code","source":["!wget https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp-workshop/master/jupyter/annotation/english/spark-nlp-basics/sample-sentences-en.txt"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":232},"colab_type":"code","id":"ggnHbf1rGn1H","outputId":"68fe0651-f683-418a-910d-b8dde093231d"},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":11},{"cell_type":"code","source":["with open('sample-sentences-en.txt') as f:\n  print (f.read())"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Peter is a very good person.\nMy life in Russia is very interesting.\nJohn and Peter are brothers. However they don&#39;t support each other that much.\nLucas Nogal Dunbercker is no longer happy. He has a good car though.\nEurope is very culture rich. There are huge churches! and big houses!\n</div>"]}}],"execution_count":12},{"cell_type":"code","source":["spark_df = spark.read.text('sample-sentences-en.txt').toDF('text')\n\nspark_df.show(truncate=False)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":194},"colab_type":"code","id":"wdrkFmcVGW-o","outputId":"026bc67e-d0ce-49a7-f45d-51889086b2c4"},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-----------------------------------------------------------------------------+\ntext                                                                         |\n+-----------------------------------------------------------------------------+\nPeter is a very good person.                                                 |\nMy life in Russia is very interesting.                                       |\nJohn and Peter are brothers. However they don&#39;t support each other that much.|\nLucas Nogal Dunbercker is no longer happy. He has a good car though.         |\nEurope is very culture rich. There are huge churches! and big houses!        |\n+-----------------------------------------------------------------------------+\n\n</div>"]}}],"execution_count":13},{"cell_type":"code","source":["spark_df.select('text').show(truncate=False)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-----------------------------------------------------------------------------+\ntext                                                                         |\n+-----------------------------------------------------------------------------+\nPeter is a very good person.                                                 |\nMy life in Russia is very interesting.                                       |\nJohn and Peter are brothers. However they don&#39;t support each other that much.|\nLucas Nogal Dunbercker is no longer happy. He has a good car though.         |\nEurope is very culture rich. There are huge churches! and big houses!        |\n+-----------------------------------------------------------------------------+\n\n</div>"]}}],"execution_count":14},{"cell_type":"code","source":["textFiles = spark.sparkContext.wholeTextFiles(\"./*.txt\",4)\n    \nspark_df = textFiles.toDF(schema=['path','text'])\n\nspark_df.show(truncate=30)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":123},"colab_type":"code","id":"s66VfRkXK9l3","outputId":"3e679be2-1388-4a54-ac9a-638a76a45133"},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-----------------------------+------------------------------+\n                         path|                          text|\n+-----------------------------+------------------------------+\ndbfs:/sample-sentences-en.txt|Peter is a very good person...|\n+-----------------------------+------------------------------+\n\n</div>"]}}],"execution_count":15},{"cell_type":"code","source":["spark_df.select('text').take(1)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[10]: [Row(text=&#34;Peter is a very good person.\\nMy life in Russia is very interesting.\\nJohn and Peter are brothers. However they don&#39;t support each other that much.\\nLucas Nogal Dunbercker is no longer happy. He has a good car though.\\nEurope is very culture rich. There are huge churches! and big houses!&#34;)]</div>"]}}],"execution_count":16},{"cell_type":"markdown","source":["### Transformers"],"metadata":{"colab_type":"text","id":"ZTcOJCrieNQK"}},{"cell_type":"markdown","source":["what are we going to do if our DataFrame doesn’t have columns in those type? Here comes transformers. In Spark NLP, we have five different transformers that are mainly used for getting the data in or transform the data from one AnnotatorType to another. Here is the list of transformers:\n\n`DocumentAssembler`: To get through the NLP process, we need to get raw data annotated. This is a special transformer that does this for us; it creates the first annotation of type Document which may be used by annotators down the road.\n\n`TokenAssembler`: This transformer reconstructs a Document type annotation from tokens, usually after these have been normalized, lemmatized, normalized, spell checked, etc, to use this document annotation in further annotators.\n\n`Doc2Chunk`: Converts DOCUMENT type annotations into CHUNK type with the contents of a chunkCol.\n\n`Chunk2Doc` : Converts a CHUNK type column back into DOCUMENT. Useful when trying to re-tokenize or do further analysis on a CHUNK result.\n\n`Finisher`: Once we have our NLP pipeline ready to go, we might want to use our annotation results somewhere else where it is easy to use. The Finisher outputs annotation(s) values into a string."],"metadata":{"colab_type":"text","id":"AwbTXq-keP7V"}},{"cell_type":"markdown","source":["each annotator accepts certain types of columns and outputs new columns in another type (we call this AnnotatorType).\n\nIn Spark NLP, we have the following types: \n\n`Document`, `token`, `chunk`, `pos`, `word_embeddings`, `date`, `entity`, `sentiment`, `named_entity`, `dependency`, `labeled_dependency`. \n\nThat is, the DataFrame you have needs to have a column from one of these types if that column will be fed into an annotator; otherwise, you’d need to use one of the Spark NLP transformers."],"metadata":{"colab_type":"text","id":"tOBEa23Odr-C"}},{"cell_type":"markdown","source":["## 2. Document Assembler"],"metadata":{"colab_type":"text","id":"ss59Lk4ULNRT"}},{"cell_type":"markdown","source":["In Spark NLP, we have five different transformers that are mainly used for getting the data in or transform the data from one AnnotatorType to another."],"metadata":{"colab_type":"text","id":"74S30BktM8p-"}},{"cell_type":"markdown","source":["That is, the DataFrame you have needs to have a column from one of these types if that column will be fed into an annotator; otherwise, you’d need to use one of the Spark NLP transformers. Here is the list of transformers: DocumentAssembler, TokenAssembler, Doc2Chunk, Chunk2Doc, and the Finisher.\n\nSo, let’s start with DocumentAssembler(), an entry point to Spark NLP annotators."],"metadata":{"colab_type":"text","id":"TtyKgt_iM_5C"}},{"cell_type":"markdown","source":["To get through the process in Spark NLP, we need to get raw data transformed into Document type at first. \n\nDocumentAssembler() is a special transformer that does this for us; it creates the first annotation of type Document which may be used by annotators down the road.\n\nDocumentAssembler() comes from sparknlp.base class and has the following settable parameters. See the full list here and the source code here.\n\n`setInputCol()` -> the name of the column that will be converted. We can specify only one column here. It can read either a String column or an Array[String]\n\n`setOutputCol()` -> optional : the name of the column in Document type that is generated. We can specify only one column here. Default is ‘document’\n\n`setIdCol()` -> optional: String type column with id information\n\n`setMetadataCol()` -> optional: Map type column with metadata information\n\n`setCleanupMode()` -> optional: Cleaning up options, \n\npossible values:\n```\ndisabled: Source kept as original. This is a default.\ninplace: removes new lines and tabs.\ninplace_full: removes new lines and tabs but also those which were converted to strings (i.e. \\n)\nshrink: removes new lines and tabs, plus merging multiple spaces and blank lines to a single space.\nshrink_full: remove new lines and tabs, including stringified values, plus shrinking spaces and blank lines.\n```"],"metadata":{"colab_type":"text","id":"GLiXAucCLWS3"}},{"cell_type":"code","source":["from sparknlp.base import *\n\ndocumentAssembler = DocumentAssembler()\\\n.setInputCol(\"text\")\\\n.setOutputCol(\"document\")\\\n.setCleanupMode(\"shrink\")\n\ndoc_df = documentAssembler.transform(spark_df)\n\ndoc_df.show(truncate=30)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":143},"colab_type":"code","id":"E76Z1SCPLOyy","outputId":"ac8ef39e-9183-43a7-a336-07f96eae43c7"},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-----------------------------+------------------------------+------------------------------+\n                         path|                          text|                      document|\n+-----------------------------+------------------------------+------------------------------+\ndbfs:/sample-sentences-en.txt|Peter is a very good person...|[[document, 0, 283, Peter i...|\n+-----------------------------+------------------------------+------------------------------+\n\n</div>"]}}],"execution_count":24},{"cell_type":"markdown","source":["At first, we define DocumentAssembler with desired parameters and then transform the data frame with it. The most important point to pay attention to here is that you need to use a String or String[Array] type column in .setInputCol(). So it doesn’t have to be named as text. You just use the column name as it is."],"metadata":{"colab_type":"text","id":"4gAyd6D0MSDF"}},{"cell_type":"code","source":["doc_df.printSchema()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":265},"colab_type":"code","id":"Ui6Ufm_fMS5h","outputId":"4477831a-37db-4eaa-9a3e-0f5f3c9adc3c"},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">root\n-- path: string (nullable = true)\n-- text: string (nullable = true)\n-- document: array (nullable = true)\n    |-- element: struct (containsNull = true)\n    |    |-- annotatorType: string (nullable = true)\n    |    |-- begin: integer (nullable = false)\n    |    |-- end: integer (nullable = false)\n    |    |-- result: string (nullable = true)\n    |    |-- metadata: map (nullable = true)\n    |    |    |-- key: string\n    |    |    |-- value: string (valueContainsNull = true)\n    |    |-- embeddings: array (nullable = true)\n    |    |    |-- element: float (containsNull = false)\n\n</div>"]}}],"execution_count":26},{"cell_type":"code","source":["doc_df.select('document.result','document.begin','document.end').show(truncate=False)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":194},"colab_type":"code","id":"UHX6sM47NIVP","outputId":"5190a80f-e073-4f78-8e6e-742904d48a20"},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----+-----+\nresult                                                                                                                                                                                                                                                                                        |begin|end  |\n+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----+-----+\n[Peter is a very good person. My life in Russia is very interesting. John and Peter are brothers. However they don&#39;t support each other that much. Lucas Nogal Dunbercker is no longer happy. He has a good car though. Europe is very culture rich. There are huge churches! and big houses!]|[0]  |[283]|\n+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----+-----+\n\n</div>"]}}],"execution_count":27},{"cell_type":"code","source":[""],"metadata":{"colab":{},"colab_type":"code","id":"GpJlyIFGMZIX"},"outputs":[],"execution_count":28},{"cell_type":"markdown","source":["The new column is in an array of struct type and has the parameters shown above. The annotators and transformers all come with universal metadata that would be filled down the road depending on the annotators being used. Unless you want to append other Spark NLP annotators to DocumentAssembler(), you don’t need to know what all these parameters mean for now. So we will talk about them in the following articles. You can access all these parameters with {column name}.{parameter name}.\n\nLet’s print out the first item’s result."],"metadata":{"colab_type":"text","id":"1zb-mdaNMbS5"}},{"cell_type":"code","source":["doc_df.select(\"document.result\").take(1)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":34},"colab_type":"code","id":"r-EWE7TIMb69","outputId":"e3679ba1-ff8a-4e21-a345-8686b1e9fcce"},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[14]: [Row(result=[&#34;Peter is a very good person. My life in Russia is very interesting. John and Peter are brothers. However they don&#39;t support each other that much. Lucas Nogal Dunbercker is no longer happy. He has a good car though. Europe is very culture rich. There are huge churches! and big houses!&#34;])]</div>"]}}],"execution_count":30},{"cell_type":"markdown","source":["If we would like to flatten the document column, we can do as follows."],"metadata":{"colab_type":"text","id":"SiogyzI-MjsI"}},{"cell_type":"code","source":["import pyspark.sql.functions as F\n\ndoc_df.withColumn(\n    \"tmp\", \n    F.explode(\"document\"))\\\n    .select(\"tmp.*\")\\\n    .show(truncate=False)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":212},"colab_type":"code","id":"ocMbESMGMeJA","outputId":"0592c808-fa80-48f4-8e05-60a3286158cf"},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-------------+-----+---+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------+----------+\nannotatorType|begin|end|result                                                                                                                                                                                                                                                                                      |metadata       |embeddings|\n+-------------+-----+---+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------+----------+\ndocument     |0    |283|Peter is a very good person. My life in Russia is very interesting. John and Peter are brothers. However they don&#39;t support each other that much. Lucas Nogal Dunbercker is no longer happy. He has a good car though. Europe is very culture rich. There are huge churches! and big houses!|[sentence -&gt; 0]|[]        |\n+-------------+-----+---+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------+----------+\n\n</div>"]}}],"execution_count":32},{"cell_type":"markdown","source":["## 3. Sentence Detector"],"metadata":{"colab_type":"text","id":"yYxUBF6vMl3o"}},{"cell_type":"markdown","source":["Finds sentence bounds in raw text."],"metadata":{"colab_type":"text","id":"H8YL-VNMcfQx"}},{"cell_type":"markdown","source":["`setCustomBounds(string)`: Custom sentence separator text\n\n`setUseCustomOnly(bool)`: Use only custom bounds without considering those of Pragmatic Segmenter. Defaults to false. Needs customBounds.\n\n`setUseAbbreviations(bool)`: Whether to consider abbreviation strategies for better accuracy but slower performance. Defaults to true.\n\n`setExplodeSentences(bool)`: Whether to split sentences into different Dataset rows. Useful for higher parallelism in fat rows. Defaults to false."],"metadata":{"colab_type":"text","id":"NeFCrak7chlb"}},{"cell_type":"code","source":["from sparknlp.annotator import *\n\n# we feed the document column coming from Document Assembler\n\nsentenceDetector = SentenceDetector().\\\nsetInputCols(['document']).\\\nsetOutputCol('sentences')\n"],"metadata":{"colab":{},"colab_type":"code","id":"hfMNS_fXb3mx"},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":36},{"cell_type":"code","source":["sent_df = sentenceDetector.transform(doc_df)\n\nsent_df.show(truncate=False)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":214},"colab_type":"code","id":"yMcJmxwyeii3","outputId":"be0d10d3-266d-457b-a20b-642797a516f7"},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-----------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\npath                         |text                                                                                                                                                                                                                                                                                        |document                                                                                                                                                                                                                                                                                                                               |sentences                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |\n+-----------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\ndbfs:/sample-sentences-en.txt|Peter is a very good person.\nMy life in Russia is very interesting.\nJohn and Peter are brothers. However they don&#39;t support each other that much.\nLucas Nogal Dunbercker is no longer happy. He has a good car though.\nEurope is very culture rich. There are huge churches! and big houses!|[[document, 0, 283, Peter is a very good person. My life in Russia is very interesting. John and Peter are brothers. However they don&#39;t support each other that much. Lucas Nogal Dunbercker is no longer happy. He has a good car though. Europe is very culture rich. There are huge churches! and big houses!, [sentence -&gt; 0], []]]|[[document, 0, 27, Peter is a very good person., [sentence -&gt; 0], []], [document, 29, 66, My life in Russia is very interesting., [sentence -&gt; 1], []], [document, 68, 95, John and Peter are brothers., [sentence -&gt; 2], []], [document, 97, 144, However they don&#39;t support each other that much., [sentence -&gt; 3], []], [document, 146, 187, Lucas Nogal Dunbercker is no longer happy., [sentence -&gt; 4], []], [document, 189, 213, He has a good car though., [sentence -&gt; 5], []], [document, 215, 242, Europe is very culture rich., [sentence -&gt; 6], []], [document, 244, 267, There are huge churches!, [sentence -&gt; 7], []], [document, 269, 283, and big houses!, [sentence -&gt; 8], []]]|\n+-----------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n\n</div>"]}}],"execution_count":37},{"cell_type":"code","source":["sent_df.select('sentences').take(3)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":34},"colab_type":"code","id":"uJrXlWNWfSs2","outputId":"27f8f5e5-94fb-43ea-9980-05feba4ec6b5"},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[18]: [Row(sentences=[Row(annotatorType=&#39;document&#39;, begin=0, end=27, result=&#39;Peter is a very good person.&#39;, metadata={&#39;sentence&#39;: &#39;0&#39;}, embeddings=[]), Row(annotatorType=&#39;document&#39;, begin=29, end=66, result=&#39;My life in Russia is very interesting.&#39;, metadata={&#39;sentence&#39;: &#39;1&#39;}, embeddings=[]), Row(annotatorType=&#39;document&#39;, begin=68, end=95, result=&#39;John and Peter are brothers.&#39;, metadata={&#39;sentence&#39;: &#39;2&#39;}, embeddings=[]), Row(annotatorType=&#39;document&#39;, begin=97, end=144, result=&#34;However they don&#39;t support each other that much.&#34;, metadata={&#39;sentence&#39;: &#39;3&#39;}, embeddings=[]), Row(annotatorType=&#39;document&#39;, begin=146, end=187, result=&#39;Lucas Nogal Dunbercker is no longer happy.&#39;, metadata={&#39;sentence&#39;: &#39;4&#39;}, embeddings=[]), Row(annotatorType=&#39;document&#39;, begin=189, end=213, result=&#39;He has a good car though.&#39;, metadata={&#39;sentence&#39;: &#39;5&#39;}, embeddings=[]), Row(annotatorType=&#39;document&#39;, begin=215, end=242, result=&#39;Europe is very culture rich.&#39;, metadata={&#39;sentence&#39;: &#39;6&#39;}, embeddings=[]), Row(annotatorType=&#39;document&#39;, begin=244, end=267, result=&#39;There are huge churches!&#39;, metadata={&#39;sentence&#39;: &#39;7&#39;}, embeddings=[]), Row(annotatorType=&#39;document&#39;, begin=269, end=283, result=&#39;and big houses!&#39;, metadata={&#39;sentence&#39;: &#39;8&#39;}, embeddings=[])])]</div>"]}}],"execution_count":38},{"cell_type":"code","source":["text ='The patient was prescribed 1 capsule of Advil for 5 days . He was seen by the endocrinology service and she was discharged on 40 units of insulin glargine at night , 12 units of insulin lispro with meals , and metformin 1000 mg two times a day . It was determined that all SGLT2 inhibitors should be discontinued indefinitely fro 3 months .'\ntext\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":54},"colab_type":"code","id":"Ly4eTKK0Th60","outputId":"ea79155b-91bb-4212-f9fa-16ce268af9cb"},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[19]: &#39;The patient was prescribed 1 capsule of Advil for 5 days . He was seen by the endocrinology service and she was discharged on 40 units of insulin glargine at night , 12 units of insulin lispro with meals , and metformin 1000 mg two times a day . It was determined that all SGLT2 inhibitors should be discontinued indefinitely fro 3 months .&#39;</div>"]}}],"execution_count":39},{"cell_type":"code","source":["spark_df = spark.createDataFrame([[text]]).toDF(\"text\")\n\nspark_df.show(truncate=False)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":143},"colab_type":"code","id":"q3wTJSVnULpX","outputId":"764e8a3d-4f3a-490b-f296-6e0634c088a6"},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\ntext                                                                                                                                                                                                                                                                                                                                                |\n+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\nThe patient was prescribed 1 capsule of Advil for 5 days . He was seen by the endocrinology service and she was discharged on 40 units of insulin glargine at night , 12 units of insulin lispro with meals , and metformin 1000 mg two times a day . It was determined that all SGLT2 inhibitors should be discontinued indefinitely fro 3 months .|\n+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n\n</div>"]}}],"execution_count":40},{"cell_type":"code","source":["spark_df.show(truncate=50)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":123},"colab_type":"code","id":"bkF2C2_GVK2S","outputId":"4e969443-dfb6-4d06-cf00-5ee87ecc1fc7"},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+--------------------------------------------------+\n                                              text|\n+--------------------------------------------------+\nThe patient was prescribed 1 capsule of Advil f...|\n+--------------------------------------------------+\n\n</div>"]}}],"execution_count":41},{"cell_type":"code","source":["doc_df = documentAssembler.transform(spark_df)\n\nsent_df = sentenceDetector.transform(doc_df)\n\nsent_df.show(truncate=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":123},"colab_type":"code","id":"UASqrLkWV2sJ","outputId":"7c1472e8-3c2e-4a27-9762-6e7d082b606f"},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+--------------------+--------------------+--------------------+\n                text|            document|           sentences|\n+--------------------+--------------------+--------------------+\nThe patient was p...|[[document, 0, 33...|[[document, 0, 57...|\n+--------------------+--------------------+--------------------+\n\n</div>"]}}],"execution_count":42},{"cell_type":"code","source":["sent_df.select('sentences.result').take(1)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":54},"colab_type":"code","id":"rYSQKC4lWf_m","outputId":"77098001-698d-48bb-cd40-37c5d30455b3"},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[23]: [Row(result=[&#39;The patient was prescribed 1 capsule of Advil for 5 days .&#39;, &#39;He was seen by the endocrinology service and she was discharged on 40 units of insulin glargine at night , 12 units of insulin lispro with meals , and metformin 1000 mg two times a day .&#39;, &#39;It was determined that all SGLT2 inhibitors should be discontinued indefinitely fro 3 months .&#39;])]</div>"]}}],"execution_count":43},{"cell_type":"code","source":["sentenceDetector.setExplodeSentences(True)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":34},"colab_type":"code","id":"ZBjcZbj8WwxC","outputId":"a28c2e27-ef2b-43ad-e45e-1f27f58c6760"},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[24]: SentenceDetector_69d4eade81f8</div>"]}}],"execution_count":44},{"cell_type":"code","source":["sent_df = sentenceDetector.transform(doc_df)\n\nsent_df.show(truncate=50)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":178},"colab_type":"code","id":"hJ6b7ZEXW8UY","outputId":"d6303694-b64c-4114-fe50-be65b7b7e170"},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+--------------------------------------------------+--------------------------------------------------+--------------------------------------------------+\n                                              text|                                          document|                                         sentences|\n+--------------------------------------------------+--------------------------------------------------+--------------------------------------------------+\nThe patient was prescribed 1 capsule of Advil f...|[[document, 0, 339, The patient was prescribed ...|[[document, 0, 57, The patient was prescribed 1...|\nThe patient was prescribed 1 capsule of Advil f...|[[document, 0, 339, The patient was prescribed ...|[[document, 59, 244, He was seen by the endocri...|\nThe patient was prescribed 1 capsule of Advil f...|[[document, 0, 339, The patient was prescribed ...|[[document, 246, 339, It was determined that al...|\n+--------------------------------------------------+--------------------------------------------------+--------------------------------------------------+\n\n</div>"]}}],"execution_count":45},{"cell_type":"code","source":["sent_df.select('sentences.result').show(truncate=False)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\nresult                                                                                                                                                                                      |\n+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n[The patient was prescribed 1 capsule of Advil for 5 days .]                                                                                                                                |\n[He was seen by the endocrinology service and she was discharged on 40 units of insulin glargine at night , 12 units of insulin lispro with meals , and metformin 1000 mg two times a day .]|\n[It was determined that all SGLT2 inhibitors should be discontinued indefinitely fro 3 months .]                                                                                            |\n+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n\n</div>"]}}],"execution_count":46},{"cell_type":"code","source":["from pyspark.sql import functions as F\n\nsent_df.select(F.explode('sentences.result')).show(truncate=False)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\ncol                                                                                                                                                                                       |\n+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\nThe patient was prescribed 1 capsule of Advil for 5 days .                                                                                                                                |\nHe was seen by the endocrinology service and she was discharged on 40 units of insulin glargine at night , 12 units of insulin lispro with meals , and metformin 1000 mg two times a day .|\nIt was determined that all SGLT2 inhibitors should be discontinued indefinitely fro 3 months .                                                                                            |\n+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n\n</div>"]}}],"execution_count":47},{"cell_type":"markdown","source":["## Tokenizer"],"metadata":{"colab_type":"text","id":"ELZyacNqbgeq"}},{"cell_type":"markdown","source":["Identifies tokens with tokenization open standards. It is an **Annotator Approach, so it requires .fit()**.\n\nA few rules will help customizing it if defaults do not fit user needs.\n\nsetExceptions(StringArray): List of tokens to not alter at all. Allows composite tokens like two worded tokens that the user may not want to split.\n\n`addException(String)`: Add a single exception\n\n`setExceptionsPath(String)`: Path to txt file with list of token exceptions\n\n`caseSensitiveExceptions(bool)`: Whether to follow case sensitiveness for matching exceptions in text\n\n`contextChars(StringArray)`: List of 1 character string to rip off from tokens, such as parenthesis or question marks. Ignored if using prefix, infix or suffix patterns.\n\n`splitChars(StringArray)`: List of 1 character string to split tokens inside, such as hyphens. Ignored if using infix, prefix or suffix patterns.\n\n`splitPattern (String)`: pattern to separate from the inside of tokens. takes priority over splitChars.\nsetTargetPattern: Basic regex rule to identify a candidate for tokenization. Defaults to \\\\S+ which means anything not a space\n\n`setSuffixPattern`: Regex to identify subtokens that are in the end of the token. Regex has to end with \\\\z and must contain groups (). Each group will become a separate token within the prefix. Defaults to non-letter characters. e.g. quotes or parenthesis\n\n`setPrefixPattern`: Regex to identify subtokens that come in the beginning of the token. Regex has to start with \\\\A and must contain groups (). Each group will become a separate token within the prefix. Defaults to non-letter characters. e.g. quotes or parenthesis\n\n`addInfixPattern`: Add an extension pattern regex with groups to the top of the rules (will target first, from more specific to the more general).\n\n`minLength`: Set the minimum allowed legth for each token\n\n`maxLength`: Set the maximum allowed legth for each token"],"metadata":{"colab_type":"text","id":"tnkBJxAqbu2b"}},{"cell_type":"code","source":["tokenizer = Tokenizer() \\\n    .setInputCols([\"document\"]) \\\n    .setOutputCol(\"token\")"],"metadata":{"colab":{},"colab_type":"code","id":"AyfeGRPKNrDF"},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":50},{"cell_type":"code","source":["text = 'Peter Parker (Spiderman) is a nice guy and lives in New York but has no e-mail!'\n\nspark_df = spark.createDataFrame([[text]]).toDF(\"text\")\n"],"metadata":{"colab":{},"colab_type":"code","id":"v-GQJmCWcYsL"},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":51},{"cell_type":"code","source":["doc_df = documentAssembler.transform(spark_df)\n\ntoken_df = tokenizer.fit(doc_df).transform(doc_df)\n\ntoken_df.show(truncate=100)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":143},"colab_type":"code","id":"iAgf0v4ucraf","outputId":"6274d13c-bf3e-4589-8728-e398cdc017b4"},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------+\n                                                                           text|                                                                                            document|                                                                                               token|\n+-------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------+\nPeter Parker (Spiderman) is a nice guy and lives in New York but has no e-mail!|[[document, 0, 78, Peter Parker (Spiderman) is a nice guy and lives in New York but has no e-mail...|[[token, 0, 4, Peter, [sentence -&gt; 0], []], [token, 6, 11, Parker, [sentence -&gt; 0], []], [token, ...|\n+-------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------+\n\n</div>"]}}],"execution_count":52},{"cell_type":"code","source":["token_df.select('token.result').take(1)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":34},"colab_type":"code","id":"sQ8eoN1mdEn-","outputId":"d4558a3f-7680-45f4-cbe4-86ac1a5c28ef"},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[31]: [Row(result=[&#39;Peter&#39;, &#39;Parker&#39;, &#39;(&#39;, &#39;Spiderman&#39;, &#39;)&#39;, &#39;is&#39;, &#39;a&#39;, &#39;nice&#39;, &#39;guy&#39;, &#39;and&#39;, &#39;lives&#39;, &#39;in&#39;, &#39;New&#39;, &#39;York&#39;, &#39;but&#39;, &#39;has&#39;, &#39;no&#39;, &#39;e-mail&#39;, &#39;!&#39;])]</div>"]}}],"execution_count":53},{"cell_type":"code","source":["tokenizer = Tokenizer() \\\n    .setInputCols([\"document\"]) \\\n    .setOutputCol(\"token\") \\\n    .setSplitChars(['-']) \\\n    .setContextChars(['(', ')', '?', '!']) \\\n    .addException(\"New York\") \\\n"],"metadata":{"colab":{},"colab_type":"code","id":"JTlinLhmfKTw"},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":54},{"cell_type":"code","source":["token_df = tokenizer.fit(doc_df).transform(doc_df)\n\ntoken_df.select('token.result').take(1)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":34},"colab_type":"code","id":"KY5V7FcXfSrs","outputId":"5f06c253-820e-4e87-c0a4-c803940c7a8e"},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[33]: [Row(result=[&#39;Peter&#39;, &#39;Parker&#39;, &#39;(&#39;, &#39;Spiderman&#39;, &#39;)&#39;, &#39;is&#39;, &#39;a&#39;, &#39;nice&#39;, &#39;guy&#39;, &#39;and&#39;, &#39;lives&#39;, &#39;in&#39;, &#39;New York&#39;, &#39;but&#39;, &#39;has&#39;, &#39;no&#39;, &#39;e&#39;, &#39;mail&#39;, &#39;!&#39;])]</div>"]}}],"execution_count":55},{"cell_type":"markdown","source":["## Stacking Spark NLP Annotators in Spark ML Pipeline"],"metadata":{"colab_type":"text","id":"l_LM44ZzgYhs"}},{"cell_type":"markdown","source":["Spark NLP provides an easy API to integrate with Spark ML Pipelines and all the Spark NLP annotators and transformers can be used within Spark ML Pipelines. So, it’s better to explain Pipeline concept through Spark ML official documentation.\n\nWhat is a Pipeline anyway? In machine learning, it is common to run a sequence of algorithms to process and learn from data. \n\nApache Spark ML represents such a workflow as a Pipeline, which consists of a sequence of PipelineStages (Transformers and Estimators) to be run in a specific order.\n\nIn simple terms, a pipeline chains multiple Transformers and Estimators together to specify an ML workflow. We use Pipeline to chain multiple Transformers and Estimators together to specify our machine learning workflow.\n\nThe figure below is for the training time usage of a Pipeline."],"metadata":{"colab_type":"text","id":"bm0mUMQMhFPU"}},{"cell_type":"markdown","source":["<img src=\"https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp-workshop/master/tutorials/Certification_Trainings/Public/databricks_notebooks/images/pipeline.png\" style=\"float: left;\">"],"metadata":{"colab_type":"text","id":"jK5AAYQqhRlG"}},{"cell_type":"markdown","source":["A Pipeline is specified as a sequence of stages, and each stage is either a Transformer or an Estimator. These stages are run in order, and the input DataFrame is transformed as it passes through each stage. That is, the data are passed through the fitted pipeline in order. Each stage’s transform() method updates the dataset and passes it to the next stage. With the help of Pipelines, we can ensure that training and test data go through identical feature processing steps.\n\nNow let’s see how this can be done in Spark NLP using Annotators and Transformers. Assume that we have the following steps that need to be applied one by one on a data frame.\n\n- Split text into sentences\n- Tokenize\n\nAnd here is how we code this pipeline up in Spark NLP."],"metadata":{"colab_type":"text","id":"dwLlY7i4hhq1"}},{"cell_type":"code","source":["from pyspark.ml import Pipeline\n\ndocumentAssembler = DocumentAssembler()\\\n.setInputCol(\"text\")\\\n.setOutputCol(\"document\")\n\nsentenceDetector = SentenceDetector().\\\nsetInputCols(['document']).\\\nsetOutputCol('sentences')\n\ntokenizer = Tokenizer() \\\n    .setInputCols([\"sentences\"]) \\\n    .setOutputCol(\"token\")\n\nnlpPipeline = Pipeline(stages=[\n documentAssembler, \n sentenceDetector,\n tokenizer\n ])\n\nempty_df = spark.createDataFrame([['']]).toDF(\"text\")\n\npipelineModel = nlpPipeline.fit(empty_df)"],"metadata":{"colab":{},"colab_type":"code","id":"_2mZXDVehhDU"},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":60},{"cell_type":"code","source":["spark_df = spark.read.text('./sample-sentences-en.txt').toDF('text')\n\nspark_df.show(truncate=False)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":194},"colab_type":"code","id":"4aJBC0VifU7H","outputId":"986bcb37-3a1a-46b9-dc5f-c452332c6401"},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-----------------------------------------------------------------------------+\ntext                                                                         |\n+-----------------------------------------------------------------------------+\nPeter is a very good person.                                                 |\nMy life in Russia is very interesting.                                       |\nJohn and Peter are brothers. However they don&#39;t support each other that much.|\nLucas Nogal Dunbercker is no longer happy. He has a good car though.         |\nEurope is very culture rich. There are huge churches! and big houses!        |\n+-----------------------------------------------------------------------------+\n\n</div>"]}}],"execution_count":61},{"cell_type":"code","source":["result = pipelineModel.transform(spark_df)"],"metadata":{"colab":{},"colab_type":"code","id":"JuhTX4-Vk-cd"},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":62},{"cell_type":"code","source":["result.show(truncate=20)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":194},"colab_type":"code","id":"iaWf94QPlT51","outputId":"1247b4dc-11e6-4ace-89a0-b216c061b23b"},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+--------------------+--------------------+--------------------+--------------------+\n                text|            document|           sentences|               token|\n+--------------------+--------------------+--------------------+--------------------+\nPeter is a very g...|[[document, 0, 27...|[[document, 0, 27...|[[token, 0, 4, Pe...|\nMy life in Russia...|[[document, 0, 37...|[[document, 0, 37...|[[token, 0, 1, My...|\nJohn and Peter ar...|[[document, 0, 76...|[[document, 0, 27...|[[token, 0, 3, Jo...|\nLucas Nogal Dunbe...|[[document, 0, 67...|[[document, 0, 41...|[[token, 0, 4, Lu...|\nEurope is very cu...|[[document, 0, 68...|[[document, 0, 27...|[[token, 0, 5, Eu...|\n+--------------------+--------------------+--------------------+--------------------+\n\n</div>"]}}],"execution_count":63},{"cell_type":"code","source":["result.printSchema()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":656},"colab_type":"code","id":"zfz0_-eFlXzk","outputId":"bf147c54-a43b-4395-bf50-a90fca72b7e8"},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">root\n-- text: string (nullable = true)\n-- document: array (nullable = true)\n    |-- element: struct (containsNull = true)\n    |    |-- annotatorType: string (nullable = true)\n    |    |-- begin: integer (nullable = false)\n    |    |-- end: integer (nullable = false)\n    |    |-- result: string (nullable = true)\n    |    |-- metadata: map (nullable = true)\n    |    |    |-- key: string\n    |    |    |-- value: string (valueContainsNull = true)\n    |    |-- embeddings: array (nullable = true)\n    |    |    |-- element: float (containsNull = false)\n-- sentences: array (nullable = true)\n    |-- element: struct (containsNull = true)\n    |    |-- annotatorType: string (nullable = true)\n    |    |-- begin: integer (nullable = false)\n    |    |-- end: integer (nullable = false)\n    |    |-- result: string (nullable = true)\n    |    |-- metadata: map (nullable = true)\n    |    |    |-- key: string\n    |    |    |-- value: string (valueContainsNull = true)\n    |    |-- embeddings: array (nullable = true)\n    |    |    |-- element: float (containsNull = false)\n-- token: array (nullable = true)\n    |-- element: struct (containsNull = true)\n    |    |-- annotatorType: string (nullable = true)\n    |    |-- begin: integer (nullable = false)\n    |    |-- end: integer (nullable = false)\n    |    |-- result: string (nullable = true)\n    |    |-- metadata: map (nullable = true)\n    |    |    |-- key: string\n    |    |    |-- value: string (valueContainsNull = true)\n    |    |-- embeddings: array (nullable = true)\n    |    |    |-- element: float (containsNull = false)\n\n</div>"]}}],"execution_count":64},{"cell_type":"code","source":["result.select('sentences.result').take(3)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":90},"colab_type":"code","id":"599Y4hQsl_mF","outputId":"8b0d3baf-587f-4c14-e4d6-4725d79cb017"},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[39]: [Row(result=[&#39;Peter is a very good person.&#39;]),\n Row(result=[&#39;My life in Russia is very interesting.&#39;]),\n Row(result=[&#39;John and Peter are brothers.&#39;, &#34;However they don&#39;t support each other that much.&#34;])]</div>"]}}],"execution_count":65},{"cell_type":"code","source":["result.select('token').take(3)[2]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":54},"colab_type":"code","id":"ehzhHXu6luaF","outputId":"44e79503-1d80-4739-94fc-26809528f887"},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[40]: Row(token=[Row(annotatorType=&#39;token&#39;, begin=0, end=3, result=&#39;John&#39;, metadata={&#39;sentence&#39;: &#39;0&#39;}, embeddings=[]), Row(annotatorType=&#39;token&#39;, begin=5, end=7, result=&#39;and&#39;, metadata={&#39;sentence&#39;: &#39;0&#39;}, embeddings=[]), Row(annotatorType=&#39;token&#39;, begin=9, end=13, result=&#39;Peter&#39;, metadata={&#39;sentence&#39;: &#39;0&#39;}, embeddings=[]), Row(annotatorType=&#39;token&#39;, begin=15, end=17, result=&#39;are&#39;, metadata={&#39;sentence&#39;: &#39;0&#39;}, embeddings=[]), Row(annotatorType=&#39;token&#39;, begin=19, end=26, result=&#39;brothers&#39;, metadata={&#39;sentence&#39;: &#39;0&#39;}, embeddings=[]), Row(annotatorType=&#39;token&#39;, begin=27, end=27, result=&#39;.&#39;, metadata={&#39;sentence&#39;: &#39;0&#39;}, embeddings=[]), Row(annotatorType=&#39;token&#39;, begin=29, end=35, result=&#39;However&#39;, metadata={&#39;sentence&#39;: &#39;1&#39;}, embeddings=[]), Row(annotatorType=&#39;token&#39;, begin=37, end=40, result=&#39;they&#39;, metadata={&#39;sentence&#39;: &#39;1&#39;}, embeddings=[]), Row(annotatorType=&#39;token&#39;, begin=42, end=46, result=&#34;don&#39;t&#34;, metadata={&#39;sentence&#39;: &#39;1&#39;}, embeddings=[]), Row(annotatorType=&#39;token&#39;, begin=48, end=54, result=&#39;support&#39;, metadata={&#39;sentence&#39;: &#39;1&#39;}, embeddings=[]), Row(annotatorType=&#39;token&#39;, begin=56, end=59, result=&#39;each&#39;, metadata={&#39;sentence&#39;: &#39;1&#39;}, embeddings=[]), Row(annotatorType=&#39;token&#39;, begin=61, end=65, result=&#39;other&#39;, metadata={&#39;sentence&#39;: &#39;1&#39;}, embeddings=[]), Row(annotatorType=&#39;token&#39;, begin=67, end=70, result=&#39;that&#39;, metadata={&#39;sentence&#39;: &#39;1&#39;}, embeddings=[]), Row(annotatorType=&#39;token&#39;, begin=72, end=75, result=&#39;much&#39;, metadata={&#39;sentence&#39;: &#39;1&#39;}, embeddings=[]), Row(annotatorType=&#39;token&#39;, begin=76, end=76, result=&#39;.&#39;, metadata={&#39;sentence&#39;: &#39;1&#39;}, embeddings=[])])</div>"]}}],"execution_count":66},{"cell_type":"markdown","source":["## Normalizer"],"metadata":{"colab_type":"text","id":"42dSp9dGmtmr"}},{"cell_type":"markdown","source":["Removes all dirty characters from text following a regex pattern and transforms words based on a provided dictionary\n\n`setCleanupPatterns(patterns)`: Regular expressions list for normalization, defaults [^A-Za-z]\n\n`setLowercase(value)`: lowercase tokens, default false\n\n`setSlangDictionary(path)`: txt file with delimited words to be transformed into something else"],"metadata":{"colab_type":"text","id":"spOjcducnAsR"}},{"cell_type":"code","source":["import string\nstring.punctuation"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":34},"colab_type":"code","id":"h6XKX2l7_Jqk","outputId":"9a65bc92-6e4d-4c4a-b793-a06cb9bcb906"},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[41]: &#39;!&#34;#$%&amp;\\&#39;()*+,-./:;&lt;=&gt;?@[\\\\]^_`{|}~&#39;</div>"]}}],"execution_count":69},{"cell_type":"code","source":["from sparknlp.annotator import *\nfrom sparknlp.base import *\n\ndocumentAssembler = DocumentAssembler()\\\n.setInputCol(\"text\")\\\n.setOutputCol(\"document\")\n\ntokenizer = Tokenizer() \\\n    .setInputCols([\"document\"]) \\\n    .setOutputCol(\"token\")\n\nnormalizer = Normalizer() \\\n    .setInputCols([\"token\"]) \\\n    .setOutputCol(\"normalized\")\\\n    .setLowercase(True)\\\n    .setCleanupPatterns([\"[^\\w\\d\\s]\"]) # remove punctuations (keep alphanumeric chars)\n    # if we don't set CleanupPatterns, it will only keep alphabet letters ([^A-Za-z])\n"],"metadata":{"colab":{},"colab_type":"code","id":"Bjd00O2yl4iN"},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":70},{"cell_type":"code","source":["\nnlpPipeline = Pipeline(stages=[\n documentAssembler, \n tokenizer,\n normalizer\n ])\n\nempty_df = spark.createDataFrame([['']]).toDF(\"text\")\n\npipelineModel = nlpPipeline.fit(empty_df)"],"metadata":{"colab":{},"colab_type":"code","id":"Kkt289yFnSwL"},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":71},{"cell_type":"code","source":["result = pipelineModel.transform(spark_df)"],"metadata":{"colab":{},"colab_type":"code","id":"rXC55-ZioNcp"},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":72},{"cell_type":"code","source":["result.show(truncate=20)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":214},"colab_type":"code","id":"oUp4au5eoYrw","outputId":"138ca2ba-a536-4e6c-e315-ea2c947086f1"},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+--------------------+--------------------+--------------------+--------------------+\n                text|            document|               token|          normalized|\n+--------------------+--------------------+--------------------+--------------------+\nPeter is a very g...|[[document, 0, 27...|[[token, 0, 4, Pe...|[[token, 0, 4, pe...|\nMy life in Russia...|[[document, 0, 37...|[[token, 0, 1, My...|[[token, 0, 1, my...|\nJohn and Peter ar...|[[document, 0, 76...|[[token, 0, 3, Jo...|[[token, 0, 3, jo...|\nLucas Nogal Dunbe...|[[document, 0, 67...|[[token, 0, 4, Lu...|[[token, 0, 4, lu...|\nEurope is very cu...|[[document, 0, 68...|[[token, 0, 5, Eu...|[[token, 0, 5, eu...|\n+--------------------+--------------------+--------------------+--------------------+\n\n</div>"]}}],"execution_count":73},{"cell_type":"code","source":["result.select('normalized.result').take(3)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":90},"colab_type":"code","id":"xYQcnFVloa8R","outputId":"c19ffc1a-2ef9-46f6-ddba-416ac3cd3b1f"},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[46]: [Row(result=[&#39;peter&#39;, &#39;is&#39;, &#39;a&#39;, &#39;very&#39;, &#39;good&#39;, &#39;person&#39;]),\n Row(result=[&#39;my&#39;, &#39;life&#39;, &#39;in&#39;, &#39;russia&#39;, &#39;is&#39;, &#39;very&#39;, &#39;interesting&#39;]),\n Row(result=[&#39;john&#39;, &#39;and&#39;, &#39;peter&#39;, &#39;are&#39;, &#39;brothers&#39;, &#39;however&#39;, &#39;they&#39;, &#39;dont&#39;, &#39;support&#39;, &#39;each&#39;, &#39;other&#39;, &#39;that&#39;, &#39;much&#39;])]</div>"]}}],"execution_count":74},{"cell_type":"code","source":["result.select('normalized').take(3)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[47]: [Row(normalized=[Row(annotatorType=&#39;token&#39;, begin=0, end=4, result=&#39;peter&#39;, metadata={&#39;sentence&#39;: &#39;0&#39;}, embeddings=[]), Row(annotatorType=&#39;token&#39;, begin=6, end=7, result=&#39;is&#39;, metadata={&#39;sentence&#39;: &#39;0&#39;}, embeddings=[]), Row(annotatorType=&#39;token&#39;, begin=9, end=9, result=&#39;a&#39;, metadata={&#39;sentence&#39;: &#39;0&#39;}, embeddings=[]), Row(annotatorType=&#39;token&#39;, begin=11, end=14, result=&#39;very&#39;, metadata={&#39;sentence&#39;: &#39;0&#39;}, embeddings=[]), Row(annotatorType=&#39;token&#39;, begin=16, end=19, result=&#39;good&#39;, metadata={&#39;sentence&#39;: &#39;0&#39;}, embeddings=[]), Row(annotatorType=&#39;token&#39;, begin=21, end=26, result=&#39;person&#39;, metadata={&#39;sentence&#39;: &#39;0&#39;}, embeddings=[])]),\n Row(normalized=[Row(annotatorType=&#39;token&#39;, begin=0, end=1, result=&#39;my&#39;, metadata={&#39;sentence&#39;: &#39;0&#39;}, embeddings=[]), Row(annotatorType=&#39;token&#39;, begin=3, end=6, result=&#39;life&#39;, metadata={&#39;sentence&#39;: &#39;0&#39;}, embeddings=[]), Row(annotatorType=&#39;token&#39;, begin=8, end=9, result=&#39;in&#39;, metadata={&#39;sentence&#39;: &#39;0&#39;}, embeddings=[]), Row(annotatorType=&#39;token&#39;, begin=11, end=16, result=&#39;russia&#39;, metadata={&#39;sentence&#39;: &#39;0&#39;}, embeddings=[]), Row(annotatorType=&#39;token&#39;, begin=18, end=19, result=&#39;is&#39;, metadata={&#39;sentence&#39;: &#39;0&#39;}, embeddings=[]), Row(annotatorType=&#39;token&#39;, begin=21, end=24, result=&#39;very&#39;, metadata={&#39;sentence&#39;: &#39;0&#39;}, embeddings=[]), Row(annotatorType=&#39;token&#39;, begin=26, end=36, result=&#39;interesting&#39;, metadata={&#39;sentence&#39;: &#39;0&#39;}, embeddings=[])]),\n Row(normalized=[Row(annotatorType=&#39;token&#39;, begin=0, end=3, result=&#39;john&#39;, metadata={&#39;sentence&#39;: &#39;0&#39;}, embeddings=[]), Row(annotatorType=&#39;token&#39;, begin=5, end=7, result=&#39;and&#39;, metadata={&#39;sentence&#39;: &#39;0&#39;}, embeddings=[]), Row(annotatorType=&#39;token&#39;, begin=9, end=13, result=&#39;peter&#39;, metadata={&#39;sentence&#39;: &#39;0&#39;}, embeddings=[]), Row(annotatorType=&#39;token&#39;, begin=15, end=17, result=&#39;are&#39;, metadata={&#39;sentence&#39;: &#39;0&#39;}, embeddings=[]), Row(annotatorType=&#39;token&#39;, begin=19, end=26, result=&#39;brothers&#39;, metadata={&#39;sentence&#39;: &#39;0&#39;}, embeddings=[]), Row(annotatorType=&#39;token&#39;, begin=29, end=35, result=&#39;however&#39;, metadata={&#39;sentence&#39;: &#39;0&#39;}, embeddings=[]), Row(annotatorType=&#39;token&#39;, begin=37, end=40, result=&#39;they&#39;, metadata={&#39;sentence&#39;: &#39;0&#39;}, embeddings=[]), Row(annotatorType=&#39;token&#39;, begin=42, end=45, result=&#39;dont&#39;, metadata={&#39;sentence&#39;: &#39;0&#39;}, embeddings=[]), Row(annotatorType=&#39;token&#39;, begin=48, end=54, result=&#39;support&#39;, metadata={&#39;sentence&#39;: &#39;0&#39;}, embeddings=[]), Row(annotatorType=&#39;token&#39;, begin=56, end=59, result=&#39;each&#39;, metadata={&#39;sentence&#39;: &#39;0&#39;}, embeddings=[]), Row(annotatorType=&#39;token&#39;, begin=61, end=65, result=&#39;other&#39;, metadata={&#39;sentence&#39;: &#39;0&#39;}, embeddings=[]), Row(annotatorType=&#39;token&#39;, begin=67, end=70, result=&#39;that&#39;, metadata={&#39;sentence&#39;: &#39;0&#39;}, embeddings=[]), Row(annotatorType=&#39;token&#39;, begin=72, end=75, result=&#39;much&#39;, metadata={&#39;sentence&#39;: &#39;0&#39;}, embeddings=[])])]</div>"]}}],"execution_count":75},{"cell_type":"markdown","source":["## Stopwords Cleaner"],"metadata":{"colab_type":"text","id":"g4jmmGZRwdwo"}},{"cell_type":"markdown","source":["This annotator excludes from a sequence of strings (e.g. the output of a Tokenizer, Normalizer, Lemmatizer, and Stemmer) and drops all the stop words from the input sequences."],"metadata":{"colab_type":"text","id":"7fgcop89yIT-"}},{"cell_type":"markdown","source":["Functions:\n\n`setStopWords`: The words to be filtered out. Array[String]\n\n`setCaseSensitive`: Whether to do a case sensitive comparison over the stop words."],"metadata":{"colab_type":"text","id":"V3igj9VeySPB"}},{"cell_type":"code","source":["stopwords_cleaner = StopWordsCleaner()\\\n      .setInputCols(\"token\")\\\n      .setOutputCol(\"cleanTokens\")\\\n      .setCaseSensitive(False)\\\n      #.setStopWords([\"no\", \"without\"]) (e.g. read a list of words from a txt)\n"],"metadata":{"colab":{},"colab_type":"code","id":"E7pS3jUcoedG"},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":79},{"cell_type":"code","source":["stopwords_cleaner.getStopWords()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[49]: [&#39;i&#39;,\n &#39;me&#39;,\n &#39;my&#39;,\n &#39;myself&#39;,\n &#39;we&#39;,\n &#39;our&#39;,\n &#39;ours&#39;,\n &#39;ourselves&#39;,\n &#39;you&#39;,\n &#39;your&#39;,\n &#39;yours&#39;,\n &#39;yourself&#39;,\n &#39;yourselves&#39;,\n &#39;he&#39;,\n &#39;him&#39;,\n &#39;his&#39;,\n &#39;himself&#39;,\n &#39;she&#39;,\n &#39;her&#39;,\n &#39;hers&#39;,\n &#39;herself&#39;,\n &#39;it&#39;,\n &#39;its&#39;,\n &#39;itself&#39;,\n &#39;they&#39;,\n &#39;them&#39;,\n &#39;their&#39;,\n &#39;theirs&#39;,\n &#39;themselves&#39;,\n &#39;what&#39;,\n &#39;which&#39;,\n &#39;who&#39;,\n &#39;whom&#39;,\n &#39;this&#39;,\n &#39;that&#39;,\n &#39;these&#39;,\n &#39;those&#39;,\n &#39;am&#39;,\n &#39;is&#39;,\n &#39;are&#39;,\n &#39;was&#39;,\n &#39;were&#39;,\n &#39;be&#39;,\n &#39;been&#39;,\n &#39;being&#39;,\n &#39;have&#39;,\n &#39;has&#39;,\n &#39;had&#39;,\n &#39;having&#39;,\n &#39;do&#39;,\n &#39;does&#39;,\n &#39;did&#39;,\n &#39;doing&#39;,\n &#39;a&#39;,\n &#39;an&#39;,\n &#39;the&#39;,\n &#39;and&#39;,\n &#39;but&#39;,\n &#39;if&#39;,\n &#39;or&#39;,\n &#39;because&#39;,\n &#39;as&#39;,\n &#39;until&#39;,\n &#39;while&#39;,\n &#39;of&#39;,\n &#39;at&#39;,\n &#39;by&#39;,\n &#39;for&#39;,\n &#39;with&#39;,\n &#39;about&#39;,\n &#39;against&#39;,\n &#39;between&#39;,\n &#39;into&#39;,\n &#39;through&#39;,\n &#39;during&#39;,\n &#39;before&#39;,\n &#39;after&#39;,\n &#39;above&#39;,\n &#39;below&#39;,\n &#39;to&#39;,\n &#39;from&#39;,\n &#39;up&#39;,\n &#39;down&#39;,\n &#39;in&#39;,\n &#39;out&#39;,\n &#39;on&#39;,\n &#39;off&#39;,\n &#39;over&#39;,\n &#39;under&#39;,\n &#39;again&#39;,\n &#39;further&#39;,\n &#39;then&#39;,\n &#39;once&#39;,\n &#39;here&#39;,\n &#39;there&#39;,\n &#39;when&#39;,\n &#39;where&#39;,\n &#39;why&#39;,\n &#39;how&#39;,\n &#39;all&#39;,\n &#39;any&#39;,\n &#39;both&#39;,\n &#39;each&#39;,\n &#39;few&#39;,\n &#39;more&#39;,\n &#39;most&#39;,\n &#39;other&#39;,\n &#39;some&#39;,\n &#39;such&#39;,\n &#39;no&#39;,\n &#39;nor&#39;,\n &#39;not&#39;,\n &#39;only&#39;,\n &#39;own&#39;,\n &#39;same&#39;,\n &#39;so&#39;,\n &#39;than&#39;,\n &#39;too&#39;,\n &#39;very&#39;,\n &#39;s&#39;,\n &#39;t&#39;,\n &#39;can&#39;,\n &#39;will&#39;,\n &#39;just&#39;,\n &#39;don&#39;,\n &#39;should&#39;,\n &#39;now&#39;,\n &#34;i&#39;ll&#34;,\n &#34;you&#39;ll&#34;,\n &#34;he&#39;ll&#34;,\n &#34;she&#39;ll&#34;,\n &#34;we&#39;ll&#34;,\n &#34;they&#39;ll&#34;,\n &#34;i&#39;d&#34;,\n &#34;you&#39;d&#34;,\n &#34;he&#39;d&#34;,\n &#34;she&#39;d&#34;,\n &#34;we&#39;d&#34;,\n &#34;they&#39;d&#34;,\n &#34;i&#39;m&#34;,\n &#34;you&#39;re&#34;,\n &#34;he&#39;s&#34;,\n &#34;she&#39;s&#34;,\n &#34;it&#39;s&#34;,\n &#34;we&#39;re&#34;,\n &#34;they&#39;re&#34;,\n &#34;i&#39;ve&#34;,\n &#34;we&#39;ve&#34;,\n &#34;you&#39;ve&#34;,\n &#34;they&#39;ve&#34;,\n &#34;isn&#39;t&#34;,\n &#34;aren&#39;t&#34;,\n &#34;wasn&#39;t&#34;,\n &#34;weren&#39;t&#34;,\n &#34;haven&#39;t&#34;,\n &#34;hasn&#39;t&#34;,\n &#34;hadn&#39;t&#34;,\n &#34;don&#39;t&#34;,\n &#34;doesn&#39;t&#34;,\n &#34;didn&#39;t&#34;,\n &#34;won&#39;t&#34;,\n &#34;wouldn&#39;t&#34;,\n &#34;shan&#39;t&#34;,\n &#34;shouldn&#39;t&#34;,\n &#34;mustn&#39;t&#34;,\n &#34;can&#39;t&#34;,\n &#34;couldn&#39;t&#34;,\n &#39;cannot&#39;,\n &#39;could&#39;,\n &#34;here&#39;s&#34;,\n &#34;how&#39;s&#34;,\n &#34;let&#39;s&#34;,\n &#39;ought&#39;,\n &#34;that&#39;s&#34;,\n &#34;there&#39;s&#34;,\n &#34;what&#39;s&#34;,\n &#34;when&#39;s&#34;,\n &#34;where&#39;s&#34;,\n &#34;who&#39;s&#34;,\n &#34;why&#39;s&#34;,\n &#39;would&#39;]</div>"]}}],"execution_count":80},{"cell_type":"code","source":["documentAssembler = DocumentAssembler()\\\n.setInputCol(\"text\")\\\n.setOutputCol(\"document\")\n\ntokenizer = Tokenizer() \\\n    .setInputCols([\"document\"]) \\\n    .setOutputCol(\"token\")\n\nnlpPipeline = Pipeline(stages=[\n documentAssembler, \n tokenizer,\n stopwords_cleaner\n ])\n\nempty_df = spark.createDataFrame([['']]).toDF(\"text\")\n\npipelineModel = nlpPipeline.fit(empty_df)"],"metadata":{"colab":{},"colab_type":"code","id":"i7-YZcLRv8Y7"},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":81},{"cell_type":"code","source":["spark_df = spark.read.text('./sample-sentences-en.txt').toDF('text')\n\nresult = pipelineModel.transform(spark_df)\n\nresult.show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":214},"colab_type":"code","id":"a-KYzOuTxEVF","outputId":"89a7fc47-40ce-4b36-983a-97e06602ec41"},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+--------------------+--------------------+--------------------+--------------------+\n                text|            document|               token|         cleanTokens|\n+--------------------+--------------------+--------------------+--------------------+\nPeter is a very g...|[[document, 0, 27...|[[token, 0, 4, Pe...|[[token, 0, 4, Pe...|\nMy life in Russia...|[[document, 0, 37...|[[token, 0, 1, My...|[[token, 3, 6, li...|\nJohn and Peter ar...|[[document, 0, 76...|[[token, 0, 3, Jo...|[[token, 0, 3, Jo...|\nLucas Nogal Dunbe...|[[document, 0, 67...|[[token, 0, 4, Lu...|[[token, 0, 4, Lu...|\nEurope is very cu...|[[document, 0, 68...|[[token, 0, 5, Eu...|[[token, 0, 5, Eu...|\n+--------------------+--------------------+--------------------+--------------------+\n\n</div>"]}}],"execution_count":82},{"cell_type":"code","source":["result.select('cleanTokens.result').take(1)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[52]: [Row(result=[&#39;Peter&#39;, &#39;good&#39;, &#39;person&#39;, &#39;.&#39;])]</div>"]}}],"execution_count":83},{"cell_type":"markdown","source":["## Token Assembler"],"metadata":{}},{"cell_type":"code","source":["documentAssembler = DocumentAssembler()\\\n.setInputCol(\"text\")\\\n.setOutputCol(\"document\")\n\nsentenceDetector = SentenceDetector().\\\n    setInputCols(['document']).\\\n    setOutputCol('sentences')\n\ntokenizer = Tokenizer() \\\n    .setInputCols([\"sentences\"]) \\\n    .setOutputCol(\"token\")\n\nnormalizer = Normalizer() \\\n    .setInputCols([\"token\"]) \\\n    .setOutputCol(\"normalized\")\\\n    .setLowercase(False)\\\n\nstopwords_cleaner = StopWordsCleaner()\\\n      .setInputCols(\"normalized\")\\\n      .setOutputCol(\"cleanTokens\")\\\n      .setCaseSensitive(False)\\\n\ntokenassembler = TokenAssembler()\\\n    .setInputCols([\"sentences\", \"cleanTokens\"]) \\\n    .setOutputCol(\"clean_text\")\n\n\nnlpPipeline = Pipeline(stages=[\n     documentAssembler,\n    sentenceDetector,\n     tokenizer,\n     normalizer,\n     stopwords_cleaner,\n     tokenassembler\n ])\n\n\nempty_df = spark.createDataFrame([['']]).toDF(\"text\")\n\npipelineModel = nlpPipeline.fit(empty_df)\n\nresult = pipelineModel.transform(spark_df)\n\nresult.show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n                text|            document|           sentences|               token|          normalized|         cleanTokens|          clean_text|\n+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\nPeter is a very g...|[[document, 0, 27...|[[document, 0, 27...|[[token, 0, 4, Pe...|[[token, 0, 4, Pe...|[[token, 0, 4, Pe...|[[document, 0, 16...|\nMy life in Russia...|[[document, 0, 37...|[[document, 0, 37...|[[token, 0, 1, My...|[[token, 0, 1, My...|[[token, 3, 6, li...|[[document, 0, 22...|\nJohn and Peter ar...|[[document, 0, 76...|[[document, 0, 27...|[[token, 0, 3, Jo...|[[token, 0, 3, Jo...|[[token, 0, 3, Jo...|[[document, 0, 18...|\nLucas Nogal Dunbe...|[[document, 0, 67...|[[document, 0, 41...|[[token, 0, 4, Lu...|[[token, 0, 4, Lu...|[[token, 0, 4, Lu...|[[document, 0, 34...|\nEurope is very cu...|[[document, 0, 68...|[[document, 0, 27...|[[token, 0, 5, Eu...|[[token, 0, 5, Eu...|[[token, 0, 5, Eu...|[[document, 0, 18...|\n+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n\n</div>"]}}],"execution_count":85},{"cell_type":"code","source":["result.select('text', 'clean_text.result').take(1)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[54]: [Row(text=&#39;Peter is a very good person.&#39;, result=[&#39;Peter good person&#39;])]</div>"]}}],"execution_count":86},{"cell_type":"code","source":["# if we use TokenAssembler().setPreservePosition(True), the original borders will be preserved (dropped & unwanted chars will be replaced by spaces)\ntokenassembler.setPreservePosition(True)\nresult2 = pipelineModel.transform(spark_df)\nresult2.select('clean_text.result').take(1)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[55]: [Row(result=[&#39;Peter    good person&#39;])]</div>"]}}],"execution_count":87},{"cell_type":"code","source":["result.select('text', F.explode('clean_text.result').alias('clean_text')).show(truncate=False)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-----------------------------------------------------------------------------+-------------------------------------+\ntext                                                                         |clean_text                           |\n+-----------------------------------------------------------------------------+-------------------------------------+\nPeter is a very good person.                                                 |Peter    good person                 |\nMy life in Russia is very interesting.                                       |life  Russia   interesting           |\nJohn and Peter are brothers. However they don&#39;t support each other that much.|John  Peter  brothers                |\nJohn and Peter are brothers. However they don&#39;t support each other that much.|However  dont support    much        |\nLucas Nogal Dunbercker is no longer happy. He has a good car though.         |Lucas Nogal Dunbercker   longer happy|\nLucas Nogal Dunbercker is no longer happy. He has a good car though.         |good car though                      |\nEurope is very culture rich. There are huge churches! and big houses!        |Europe   culture rich                |\nEurope is very culture rich. There are huge churches! and big houses!        |huge churches                        |\nEurope is very culture rich. There are huge churches! and big houses!        |big houses                           |\n+-----------------------------------------------------------------------------+-------------------------------------+\n\n</div>"]}}],"execution_count":88},{"cell_type":"code","source":["import pyspark.sql.functions as F\n\nresult.withColumn(\n    \"tmp\", \n    F.explode(\"clean_text\")) \\\n    .select(\"tmp.*\").select(\"begin\",\"end\",\"result\",\"metadata.sentence\").show(truncate = False)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-----+---+-------------------------------------+--------+\nbegin|end|result                               |sentence|\n+-----+---+-------------------------------------+--------+\n0    |19 |Peter    good person                 |0       |\n0    |25 |life  Russia   interesting           |0       |\n0    |20 |John  Peter  brothers                |0       |\n29   |57 |However  dont support    much        |1       |\n0    |36 |Lucas Nogal Dunbercker   longer happy|0       |\n43   |57 |good car though                      |1       |\n0    |20 |Europe   culture rich                |0       |\n29   |41 |huge churches                        |1       |\n54   |63 |big houses                           |2       |\n+-----+---+-------------------------------------+--------+\n\n</div>"]}}],"execution_count":89},{"cell_type":"code","source":["# if we hadn't used Sentence Detector, this would be what we got. (tokenizer gets document instead of sentences column)\n\ntokenizer = Tokenizer() \\\n    .setInputCols([\"document\"]) \\\n    .setOutputCol(\"token\")\n\ntokenassembler = TokenAssembler()\\\n    .setInputCols([\"document\", \"cleanTokens\"]) \\\n    .setOutputCol(\"clean_text\")\n\nnlpPipeline = Pipeline(stages=[\n     documentAssembler,\n     tokenizer,\n     normalizer,\n     stopwords_cleaner,\n     tokenassembler\n ])\n\n\nempty_df = spark.createDataFrame([['']]).toDF(\"text\")\n\npipelineModel = nlpPipeline.fit(empty_df)\n\nresult = pipelineModel.transform(spark_df)\n\nresult.select('text', 'clean_text.result').show(truncate=False)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-----------------------------------------------------------------------------+-----------------------------------------------------+\ntext                                                                         |result                                               |\n+-----------------------------------------------------------------------------+-----------------------------------------------------+\nPeter is a very good person.                                                 |[Peter good person]                                  |\nMy life in Russia is very interesting.                                       |[life Russia interesting]                            |\nJohn and Peter are brothers. However they don&#39;t support each other that much.|[John Peter brothers However dont support much]      |\nLucas Nogal Dunbercker is no longer happy. He has a good car though.         |[Lucas Nogal Dunbercker longer happy good car though]|\nEurope is very culture rich. There are huge churches! and big houses!        |[Europe culture rich huge churches big houses]       |\n+-----------------------------------------------------------------------------+-----------------------------------------------------+\n\n</div>"]}}],"execution_count":90},{"cell_type":"code","source":["\nresult.withColumn(\n    \"tmp\", \n    F.explode(\"clean_text\")) \\\n    .select(\"tmp.*\").select(\"begin\",\"end\",\"result\",\"metadata.sentence\").show(truncate = False)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-----+---+---------------------------------------------------+--------+\nbegin|end|result                                             |sentence|\n+-----+---+---------------------------------------------------+--------+\n0    |16 |Peter good person                                  |0       |\n0    |22 |life Russia interesting                            |0       |\n0    |44 |John Peter brothers However dont support much      |0       |\n0    |50 |Lucas Nogal Dunbercker longer happy good car though|0       |\n0    |43 |Europe culture rich huge churches big houses       |0       |\n+-----+---+---------------------------------------------------+--------+\n\n</div>"]}}],"execution_count":91},{"cell_type":"markdown","source":["## Stemmer"],"metadata":{"colab_type":"text","id":"XO_ZWY2z1Ka6"}},{"cell_type":"markdown","source":["Returns hard-stems out of words with the objective of retrieving the meaningful part of the word"],"metadata":{"colab_type":"text","id":"aE2PEX0x1NgQ"}},{"cell_type":"code","source":["stemmer = Stemmer() \\\n    .setInputCols([\"token\"]) \\\n    .setOutputCol(\"stem\")"],"metadata":{"colab":{},"colab_type":"code","id":"0sN0gS3ayfHT"},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":94},{"cell_type":"code","source":["documentAssembler = DocumentAssembler()\\\n.setInputCol(\"text\")\\\n.setOutputCol(\"document\")\n\ntokenizer = Tokenizer() \\\n    .setInputCols([\"document\"]) \\\n    .setOutputCol(\"token\")\n\nnlpPipeline = Pipeline(stages=[\n documentAssembler, \n tokenizer,\n stemmer\n ])\n\nempty_df = spark.createDataFrame([['']]).toDF(\"text\")\n\npipelineModel = nlpPipeline.fit(empty_df)"],"metadata":{"colab":{},"colab_type":"code","id":"VdJ-8aUy1RrC"},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":95},{"cell_type":"code","source":["result = pipelineModel.transform(spark_df)\n\nresult.show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":214},"colab_type":"code","id":"pI9PANoL1Zi0","outputId":"55123ea2-23db-4a60-b0ed-d14c901c611a"},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+--------------------+--------------------+--------------------+--------------------+\n                text|            document|               token|                stem|\n+--------------------+--------------------+--------------------+--------------------+\nPeter is a very g...|[[document, 0, 27...|[[token, 0, 4, Pe...|[[token, 0, 4, pe...|\nMy life in Russia...|[[document, 0, 37...|[[token, 0, 1, My...|[[token, 0, 1, my...|\nJohn and Peter ar...|[[document, 0, 76...|[[token, 0, 3, Jo...|[[token, 0, 3, jo...|\nLucas Nogal Dunbe...|[[document, 0, 67...|[[token, 0, 4, Lu...|[[token, 0, 4, lu...|\nEurope is very cu...|[[document, 0, 68...|[[token, 0, 5, Eu...|[[token, 0, 5, eu...|\n+--------------------+--------------------+--------------------+--------------------+\n\n</div>"]}}],"execution_count":96},{"cell_type":"code","source":["result.select('stem.result').show(truncate=False)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":214},"colab_type":"code","id":"SvapcdcM1fWR","outputId":"1b84c21b-5d81-49d5-c352-2e20abc05946"},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-------------------------------------------------------------------------------------------+\nresult                                                                                     |\n+-------------------------------------------------------------------------------------------+\n[peter, i, a, veri, good, person, .]                                                       |\n[my, life, in, russia, i, veri, interest, .]                                               |\n[john, and, peter, ar, brother, ., howev, thei, don&#39;t, support, each, other, that, much, .]|\n[luca, nogal, dunberck, i, no, longer, happi, ., he, ha, a, good, car, though, .]          |\n[europ, i, veri, cultur, rich, ., there, ar, huge, church, !, and, big, hous, !]           |\n+-------------------------------------------------------------------------------------------+\n\n</div>"]}}],"execution_count":97},{"cell_type":"code","source":["import pyspark.sql.functions as F\n\nresult_df = result.select(F.explode(F.arrays_zip('token.result', 'stem.result')).alias(\"cols\")) \\\n.select(F.expr(\"cols['0']\").alias(\"token\"),\n        F.expr(\"cols['1']\").alias(\"stem\")).toPandas()\n\nresult_df.head(10)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":347},"colab_type":"code","id":"wCEPeXr-1iXR","outputId":"d9cd3cec-da73-4b6a-85f1-aab8d5a7b19d"},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>token</th>\n      <th>stem</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Peter</td>\n      <td>peter</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>is</td>\n      <td>i</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>a</td>\n      <td>a</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>very</td>\n      <td>veri</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>good</td>\n      <td>good</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>person</td>\n      <td>person</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>.</td>\n      <td>.</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>My</td>\n      <td>my</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>life</td>\n      <td>life</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>in</td>\n      <td>in</td>\n    </tr>\n  </tbody>\n</table>\n</div>"]}}],"execution_count":98},{"cell_type":"markdown","source":["## Lemmatizer"],"metadata":{"colab_type":"text","id":"On3UrCoM2RFQ"}},{"cell_type":"markdown","source":["Retrieves lemmas out of words with the objective of returning a base dictionary word"],"metadata":{"colab_type":"text","id":"onCYFXGO2VSk"}},{"cell_type":"code","source":["!wget https://raw.githubusercontent.com/mahavivo/vocabulary/master/lemmas/AntBNC_lemmas_ver_001.txt -P /FileStore/"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":232},"colab_type":"code","id":"gZYXURzi3N2T","outputId":"915f8a1b-113e-4630-b132-e67c72e88415"},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":101},{"cell_type":"code","source":["lemmatizer = Lemmatizer() \\\n    .setInputCols([\"token\"]) \\\n    .setOutputCol(\"lemma\") \\\n    .setDictionary(\"/FileStore/AntBNC_lemmas_ver_001.txt\", value_delimiter =\"\\t\", key_delimiter = \"->\")"],"metadata":{"colab":{},"colab_type":"code","id":"2y_woCty2QXj"},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":102},{"cell_type":"code","source":["documentAssembler = DocumentAssembler()\\\n.setInputCol(\"text\")\\\n.setOutputCol(\"document\")\n\ntokenizer = Tokenizer() \\\n    .setInputCols([\"document\"]) \\\n    .setOutputCol(\"token\")\n\nstemmer = Stemmer() \\\n    .setInputCols([\"token\"]) \\\n    .setOutputCol(\"stem\")\n\nnlpPipeline = Pipeline(stages=[\n documentAssembler, \n tokenizer,\n stemmer,\n lemmatizer\n ])\n\nempty_df = spark.createDataFrame([['']]).toDF(\"text\")\n\npipelineModel = nlpPipeline.fit(empty_df)"],"metadata":{"colab":{},"colab_type":"code","id":"dxSeQ0yz16cv"},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":103},{"cell_type":"code","source":["result = pipelineModel.transform(spark_df)\n\nresult.show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":214},"colab_type":"code","id":"AM1K7xcp3h_B","outputId":"96742d5e-b93c-44a6-ec05-c3738f58c122"},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+--------------------+--------------------+--------------------+--------------------+--------------------+\n                text|            document|               token|                stem|               lemma|\n+--------------------+--------------------+--------------------+--------------------+--------------------+\nPeter is a very g...|[[document, 0, 27...|[[token, 0, 4, Pe...|[[token, 0, 4, pe...|[[token, 0, 4, Pe...|\nMy life in Russia...|[[document, 0, 37...|[[token, 0, 1, My...|[[token, 0, 1, my...|[[token, 0, 1, My...|\nJohn and Peter ar...|[[document, 0, 76...|[[token, 0, 3, Jo...|[[token, 0, 3, jo...|[[token, 0, 3, Jo...|\nLucas Nogal Dunbe...|[[document, 0, 67...|[[token, 0, 4, Lu...|[[token, 0, 4, lu...|[[token, 0, 4, Lu...|\nEurope is very cu...|[[document, 0, 68...|[[token, 0, 5, Eu...|[[token, 0, 5, eu...|[[token, 0, 5, Eu...|\n+--------------------+--------------------+--------------------+--------------------+--------------------+\n\n</div>"]}}],"execution_count":104},{"cell_type":"code","source":["result.select('lemma.result').show(truncate=False)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":214},"colab_type":"code","id":"3LVBY-fL3kmv","outputId":"47e26827-d422-45e9-c777-67b545119253"},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+---------------------------------------------------------------------------------------------+\nresult                                                                                       |\n+---------------------------------------------------------------------------------------------+\n[Peter, be, a, very, good, person, .]                                                        |\n[My, life, in, Russia, be, very, interest, .]                                                |\n[John, and, Peter, be, brother, ., However, they, don&#39;t, support, each, other, that, much, .]|\n[Lucas, Nogal, Dunbercker, be, no, long, happy, ., He, have, a, good, car, though, .]        |\n[Europe, be, very, culture, rich, ., There, be, huge, church, !, and, big, house, !]         |\n+---------------------------------------------------------------------------------------------+\n\n</div>"]}}],"execution_count":105},{"cell_type":"code","source":["result_df = result.select(F.explode(F.arrays_zip('token.result', 'stem.result',  'lemma.result')).alias(\"cols\")) \\\n.select(F.expr(\"cols['0']\").alias(\"token\"),\n        F.expr(\"cols['1']\").alias(\"stem\"),\n        F.expr(\"cols['2']\").alias(\"lemma\")).toPandas()\n\nresult_df.head(10)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":347},"colab_type":"code","id":"QAJCpPbW3oZq","outputId":"e1cb38cf-21f5-47b5-9d0a-dfae3cffb833"},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>token</th>\n      <th>stem</th>\n      <th>lemma</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Peter</td>\n      <td>peter</td>\n      <td>Peter</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>is</td>\n      <td>i</td>\n      <td>be</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>a</td>\n      <td>a</td>\n      <td>a</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>very</td>\n      <td>veri</td>\n      <td>very</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>good</td>\n      <td>good</td>\n      <td>good</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>person</td>\n      <td>person</td>\n      <td>person</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>.</td>\n      <td>.</td>\n      <td>.</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>My</td>\n      <td>my</td>\n      <td>My</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>life</td>\n      <td>life</td>\n      <td>life</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>in</td>\n      <td>in</td>\n      <td>in</td>\n    </tr>\n  </tbody>\n</table>\n</div>"]}}],"execution_count":106},{"cell_type":"markdown","source":["## NGram Generator"],"metadata":{}},{"cell_type":"markdown","source":["NGramGenerator annotator takes as input a sequence of strings (e.g. the output of a `Tokenizer`, `Normalizer`, `Stemmer`, `Lemmatizer`, and `StopWordsCleaner`). \n\nThe parameter n is used to determine the number of terms in each n-gram. The output will consist of a sequence of n-grams where each n-gram is represented by a space-delimited string of n consecutive words with annotatorType `CHUNK` same as the Chunker annotator.\n\nFunctions:\n\n`setN:` number elements per n-gram (>=1)\n\n`setEnableCumulative:` whether to calculate just the actual n-grams or all n-grams from 1 through n\n\n`setDelimiter:` Glue character used to join the tokens"],"metadata":{}},{"cell_type":"code","source":["ngrams_cum = NGramGenerator() \\\n            .setInputCols([\"token\"]) \\\n            .setOutputCol(\"ngrams\") \\\n            .setN(3) \\\n            .setEnableCumulative(True)\\\n            .setDelimiter(\"_\") # Default is space\n    \n# .setN(3) means, take bigrams and trigrams.\n\nnlpPipeline = Pipeline(stages=[\n documentAssembler, \n tokenizer,\n ngrams_cum\n ])\n\nempty_df = spark.createDataFrame([['']]).toDF(\"text\")\n\npipelineModel = nlpPipeline.fit(empty_df)\n\nresult = pipelineModel.transform(spark_df)\n\nresult.select('ngrams.result').show(truncate=200)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n                                                                                                                                                                                                  result|\n+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n                                    [Peter, is, a, very, good, person, ., Peter_is, is_a, a_very, very_good, good_person, person_., Peter_is_a, is_a_very, a_very_good, very_good_person, good_person_.]|\n[My, life, in, Russia, is, very, interesting, ., My_life, life_in, in_Russia, Russia_is, is_very, very_interesting, interesting_., My_life_in, life_in_Russia, in_Russia_is, Russia_is_very, is_very_...|\n[John, and, Peter, are, brothers, ., However, they, don&#39;t, support, each, other, that, much, ., John_and, and_Peter, Peter_are, are_brothers, brothers_., ._However, However_they, they_don&#39;t, don&#39;t_...|\n[Lucas, Nogal, Dunbercker, is, no, longer, happy, ., He, has, a, good, car, though, ., Lucas_Nogal, Nogal_Dunbercker, Dunbercker_is, is_no, no_longer, longer_happy, happy_., ._He, He_has, has_a, a_...|\n[Europe, is, very, culture, rich, ., There, are, huge, churches, !, and, big, houses, !, Europe_is, is_very, very_culture, culture_rich, rich_., ._There, There_are, are_huge, huge_churches, churche...|\n+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n\n</div>"]}}],"execution_count":109},{"cell_type":"code","source":["ngrams_nonCum = NGramGenerator() \\\n            .setInputCols([\"token\"]) \\\n            .setOutputCol(\"ngrams_v2\") \\\n            .setN(3) \\\n            .setEnableCumulative(False)\\\n            .setDelimiter(\"_\") # Default is space\n    \nngrams_nonCum.transform(result).select('ngrams_v2.result').show(truncate=200)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n                                                                                                                                                                                                  result|\n+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n                                                                                                                                   [Peter_is_a, is_a_very, a_very_good, very_good_person, good_person_.]|\n                                                                                                     [My_life_in, life_in_Russia, in_Russia_is, Russia_is_very, is_very_interesting, very_interesting_.]|\n[John_and_Peter, and_Peter_are, Peter_are_brothers, are_brothers_., brothers_._However, ._However_they, However_they_don&#39;t, they_don&#39;t_support, don&#39;t_support_each, support_each_other, each_other_th...|\n   [Lucas_Nogal_Dunbercker, Nogal_Dunbercker_is, Dunbercker_is_no, is_no_longer, no_longer_happy, longer_happy_., happy_._He, ._He_has, He_has_a, has_a_good, a_good_car, good_car_though, car_though_.]|\n[Europe_is_very, is_very_culture, very_culture_rich, culture_rich_., rich_._There, ._There_are, There_are_huge, are_huge_churches, huge_churches_!, churches_!_and, !_and_big, and_big_houses, big_ho...|\n+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n\n</div>"]}}],"execution_count":110},{"cell_type":"markdown","source":["## TextMatcher"],"metadata":{"colab_type":"text","id":"C55M47nKCL3E"}},{"cell_type":"markdown","source":["Annotator to match entire phrases (by token) provided in a file against a Document\n\nFunctions:\n\nsetEntities(path, format, options): Provides a file with phrases to match. Default: Looks up path in configuration.\n\npath: a path to a file that contains the entities in the specified format.\n\nreadAs: the format of the file, can be one of {ReadAs.LINE_BY_LINE, ReadAs.SPARK_DATASET}. Defaults to LINE_BY_LINE.\n\noptions: a map of additional parameters. Defaults to {“format”: “text”}.\n\nentityValue : Value for the entity metadata field to indicate which chunk comes from which textMatcher when there are multiple textMatchers.\n\nmergeOverlapping : whether to merge overlapping matched chunks. Defaults false\n\ncaseSensitive : whether to match regardless of case. Defaults true"],"metadata":{"colab_type":"text","id":"8hLITrkJICKO"}},{"cell_type":"code","source":["# first method for doing this, second option below\nimport urllib.request\nwith urllib.request.urlopen('https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/resources/en/pubmed/pubmed-sample.csv') as f: \n    content = f.read().decode('utf-8')\n    dbutils.fs.put(\"/dbfs/tmp/pubmed/pubmed-sample.csv\", content)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":232},"colab_type":"code","id":"oDXgwTBb4OBj","outputId":"5a427427-bab4-49e5-d183-3fb275f9f3fb"},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Wrote 10484510 bytes.\n</div>"]}}],"execution_count":113},{"cell_type":"code","source":["%sh\nTMP=/dbfs/tmp/pubmed\nif [ ! -d \"$TMP\" ]; then\n    mkdir $TMP\n    cd $TMP\n    wget https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/resources/en/pubmed/pubmed-sample.csv\nfi"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">--2020-08-18 02:41:14--  https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/resources/en/pubmed/pubmed-sample.csv\nResolving s3.amazonaws.com (s3.amazonaws.com)... 52.216.141.94\nConnecting to s3.amazonaws.com (s3.amazonaws.com)|52.216.141.94|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 10484510 (10.0M) [text/csv]\nSaving to: ‘pubmed-sample.csv’\n\n     0K .......... .......... .......... .......... ..........  0%  559K 18s\n    50K .......... .......... .......... .......... ..........  0% 1.09M 14s\n   100K .......... .......... .......... .......... ..........  1%  596K 15s\n   150K .......... .......... .......... .......... ..........  1% 17.4M 11s\n   200K .......... .......... .......... .......... ..........  2% 21.6M 9s\n   250K .......... .......... .......... .......... ..........  2%  664K 10s\n   300K .......... .......... .......... .......... ..........  3% 18.8M 9s\n   350K .......... .......... .......... .......... ..........  3% 21.4M 7s\n   400K .......... .......... .......... .......... ..........  4% 22.6M 7s\n   450K .......... .......... .......... .......... ..........  4% 24.8M 6s\n   500K .......... .......... .......... .......... ..........  5%  763K 7s\n   550K .......... .......... .......... .......... ..........  5% 8.80M 6s\n   600K .......... .......... .......... .......... ..........  6% 16.9M 6s\n   650K .......... .......... .......... .......... ..........  6% 22.1M 5s\n   700K .......... .......... .......... .......... ..........  7% 22.6M 5s\n   750K .......... .......... .......... .......... ..........  7% 1.82M 5s\n   800K .......... .......... .......... .......... ..........  8% 14.0M 5s\n   850K .......... .......... .......... .......... ..........  8% 22.9M 4s\n   900K .......... .......... .......... .......... ..........  9% 24.8M 4s\n   950K .......... .......... .......... .......... ..........  9% 21.1M 4s\n  1000K .......... .......... .......... .......... .......... 10% 20.5M 4s\n  1050K .......... .......... .......... .......... .......... 10% 1.69M 4s\n  1100K .......... .......... .......... .......... .......... 11% 9.97M 4s\n  1150K .......... .......... .......... .......... .......... 11% 18.5M 3s\n  1200K .......... .......... .......... .......... .......... 12% 20.2M 3s\n  1250K .......... .......... .......... .......... .......... 12% 19.3M 3s\n  1300K .......... .......... .......... .......... .......... 13% 25.3M 3s\n  1350K .......... .......... .......... .......... .......... 13% 28.4M 3s\n  1400K .......... .......... .......... .......... .......... 14% 27.6M 3s\n  1450K .......... .......... .......... .......... .......... 14% 25.5M 3s\n  1500K .......... .......... .......... .......... .......... 15% 24.0M 3s\n  1550K .......... .......... .......... .......... .......... 15% 25.8M 3s\n  1600K .......... .......... .......... .......... .......... 16% 3.05M 3s\n  1650K .......... .......... .......... .......... .......... 16% 16.3M 2s\n  1700K .......... .......... .......... .......... .......... 17% 23.3M 2s\n  1750K .......... .......... .......... .......... .......... 17% 27.7M 2s\n  1800K .......... .......... .......... .......... .......... 18% 26.6M 2s\n  1850K .......... .......... .......... .......... .......... 18% 26.7M 2s\n  1900K .......... .......... .......... .......... .......... 19% 24.7M 2s\n  1950K .......... .......... .......... .......... .......... 19% 27.4M 2s\n  2000K .......... .......... .......... .......... .......... 20% 25.4M 2s\n  2050K .......... .......... .......... .......... .......... 20% 26.1M 2s\n  2100K .......... .......... .......... .......... .......... 20% 22.8M 2s\n  2150K .......... .......... .......... .......... .......... 21% 26.5M 2s\n  2200K .......... .......... .......... .......... .......... 21% 2.49M 2s\n  2250K .......... .......... .......... .......... .......... 22% 12.7M 2s\n  2300K .......... .......... .......... .......... .......... 22% 17.1M 2s\n  2350K .......... .......... .......... .......... .......... 23% 28.5M 2s\n  2400K .......... .......... .......... .......... .......... 23% 27.2M 2s\n  2450K .......... .......... .......... .......... .......... 24% 28.2M 2s\n  2500K .......... .......... .......... .......... .......... 24% 23.6M 2s\n  2550K .......... .......... .......... .......... .......... 25% 25.1M 2s\n  2600K .......... .......... .......... .......... .......... 25% 26.2M 2s\n  2650K .......... .......... .......... .......... .......... 26% 26.0M 2s\n  2700K .......... .......... .......... .......... .......... 26% 22.9M 2s\n  2750K .......... .......... .......... .......... .......... 27% 26.2M 1s\n  2800K .......... .......... .......... .......... .......... 27% 4.05M 1s\n  2850K .......... .......... .......... .......... .......... 28% 21.5M 1s\n  2900K .......... .......... .......... .......... .......... 28% 17.1M 1s\n  2950K .......... .......... .......... .......... .......... 29% 25.1M 1s\n  3000K .......... .......... .......... .......... .......... 29% 24.1M 1s\n  3050K .......... .......... .......... .......... .......... 30% 27.2M 1s\n  3100K .......... .......... .......... .......... .......... 30% 21.2M 1s\n  3150K .......... .......... .......... .......... .......... 31% 26.9M 1s\n  3200K .......... .......... .......... .......... .......... 31% 27.5M 1s\n  3250K .......... .......... .......... .......... .......... 32% 28.9M 1s\n  3300K .......... .......... .......... .......... .......... 32% 22.2M 1s\n  3350K .......... .......... .......... .......... .......... 33% 2.15M 1s\n  3400K .......... .......... .......... .......... .......... 33% 17.2M 1s\n  3450K .......... .......... .......... .......... .......... 34% 24.3M 1s\n  3500K .......... .......... .......... .......... .......... 34% 21.7M 1s\n  3550K .......... .......... .......... .......... .......... 35% 27.3M 1s\n  3600K .......... .......... .......... .......... .......... 35% 24.7M 1s\n  3650K .......... .......... .......... .......... .......... 36% 26.8M 1s\n  3700K .......... .......... .......... .......... .......... 36% 23.4M 1s\n  3750K .......... .......... .......... .......... .......... 37% 26.1M 1s\n  3800K .......... .......... .......... .......... .......... 37% 26.1M 1s\n  3850K .......... .......... .......... .......... .......... 38% 22.8M 1s\n  3900K .......... .......... .......... .......... .......... 38% 22.7M 1s\n  3950K .......... .......... .......... .......... .......... 39% 27.2M 1s\n  4000K .......... .......... .......... .......... .......... 39% 3.05M 1s\n  4050K .......... .......... .......... .......... .......... 40% 25.8M 1s\n  4100K .......... .......... .......... .......... .......... 40% 21.5M 1s\n  4150K .......... .......... .......... .......... .......... 41% 25.7M 1s\n  4200K .......... .......... .......... .......... .......... 41% 26.6M 1s\n  4250K .......... .......... .......... .......... .......... 41% 26.2M 1s\n  4300K .......... .......... .......... .......... .......... 42% 23.5M 1s\n  4350K .......... .......... .......... .......... .......... 42% 22.5M 1s\n  4400K .......... .......... .......... .......... .......... 43% 28.6M 1s\n  4450K .......... .......... .......... .......... .......... 43% 27.4M 1s\n  4500K .......... .......... .......... .......... .......... 44% 21.2M 1s\n  4550K .......... .......... .......... .......... .......... 44% 27.3M 1s\n  4600K .......... .......... .......... .......... .......... 45% 26.2M 1s\n  4650K .......... .......... .......... .......... .......... 45% 26.8M 1s\n  4700K .......... .......... .......... .......... .......... 46% 23.8M 1s\n  4750K .......... .......... .......... .......... .......... 46% 27.1M 1s\n  4800K .......... .......... .......... .......... .......... 47% 26.9M 1s\n  4850K .......... .......... .......... .......... .......... 47% 25.4M 1s\n  4900K .......... .......... .......... .......... .......... 48% 7.15M 1s\n  4950K .......... .......... .......... .......... .......... 48% 18.8M 1s\n  5000K .......... .......... .......... .......... .......... 49% 22.7M 1s\n  5050K .......... .......... .......... .......... .......... 49% 26.1M 1s\n  5100K .......... .......... .......... .......... .......... 50% 1.44M 1s\n  5150K .......... .......... .......... .......... .......... 50% 22.9M 1s\n  5200K .......... .......... .......... .......... .......... 51% 24.3M 1s\n  5250K .......... .......... .......... .......... .......... 51% 19.2M 1s\n  5300K .......... .......... .......... .......... .......... 52% 9.58M 1s\n  5350K .......... .......... .......... .......... .......... 52% 22.1M 1s\n  5400K .......... .......... .......... .......... .......... 53% 22.1M 1s\n  5450K .......... .......... .......... .......... .......... 53% 23.9M 1s\n  5500K .......... .......... .......... .......... .......... 54% 19.2M 1s\n  5550K .......... .......... .......... .......... .......... 54% 24.5M 1s\n  5600K .......... .......... .......... .......... .......... 55% 25.2M 1s\n  5650K .......... .......... .......... .......... .......... 55% 21.9M 1s\n  5700K .......... .......... .......... .......... .......... 56% 21.4M 1s\n  5750K .......... .......... .......... .......... .......... 56% 25.0M 1s\n  5800K .......... .......... .......... .......... .......... 57% 27.2M 1s\n  5850K .......... .......... .......... .......... .......... 57% 28.0M 1s\n  5900K .......... .......... .......... .......... .......... 58% 19.7M 1s\n  5950K .......... .......... .......... .......... .......... 58% 28.5M 1s\n  6000K .......... .......... .......... .......... .......... 59% 16.8M 1s\n  6050K .......... .......... .......... .......... .......... 59% 21.0M 1s\n  6100K .......... .......... .......... .......... .......... 60% 10.0M 1s\n  6150K .......... .......... .......... .......... .......... 60% 16.1M 1s\n  6200K .......... .......... .......... .......... .......... 61% 16.4M 1s\n  6250K .......... .......... .......... .......... .......... 61% 15.8M 0s\n  6300K .......... .......... .......... .......... .......... 62% 17.6M 0s\n  6350K .......... .......... .......... .......... .......... 62% 18.6M 0s\n  6400K .......... .......... .......... .......... .......... 62% 16.9M 0s\n  6450K .......... .......... .......... .......... .......... 63% 17.3M 0s\n  6500K .......... .......... .......... .......... .......... 63% 20.4M 0s\n  6550K .......... .......... .......... .......... .......... 64% 16.6M 0s\n  6600K .......... .......... .......... .......... .......... 64% 21.9M 0s\n  6650K .......... .......... .......... .......... .......... 65% 20.1M 0s\n  6700K .......... .......... .......... .......... .......... 65% 19.3M 0s\n  6750K .......... .......... .......... .......... .......... 66% 21.3M 0s\n  6800K .......... .......... .......... .......... .......... 66% 20.1M 0s\n  6850K .......... .......... .......... .......... .......... 67% 26.2M 0s\n  6900K .......... .......... .......... .......... .......... 67% 21.3M 0s\n  6950K .......... .......... .......... .......... .......... 68% 23.8M 0s\n  7000K .......... .......... .......... .......... .......... 68% 28.3M 0s\n  7050K .......... .......... .......... .......... .......... 69% 28.1M 0s\n  7100K .......... .......... .......... .......... .......... 69% 23.9M 0s\n  7150K .......... .......... .......... .......... .......... 70% 25.4M 0s\n  7200K .......... .......... .......... .......... .......... 70% 27.1M 0s\n  7250K .......... .......... .......... .......... .......... 71% 28.3M 0s\n  7300K .......... .......... .......... .......... .......... 71% 23.2M 0s\n  7350K .......... .......... .......... .......... .......... 72% 29.2M 0s\n  7400K .......... .......... .......... .......... .......... 72% 26.9M 0s\n  7450K .......... .......... .......... .......... .......... 73% 26.7M 0s\n  7500K .......... .......... .......... .......... .......... 73% 23.2M 0s\n  7550K .......... .......... .......... .......... .......... 74% 28.1M 0s\n  7600K .......... .......... .......... .......... .......... 74% 27.8M 0s\n  7650K .......... .......... .......... .......... .......... 75% 26.2M 0s\n  7700K .......... .......... .......... .......... .......... 75% 12.5M 0s\n  7750K .......... .......... .......... .......... .......... 76% 21.1M 0s\n  7800K .......... .......... .......... .......... .......... 76% 27.7M 0s\n  7850K .......... .......... .......... .......... .......... 77% 26.5M 0s\n  7900K .......... .......... .......... .......... .......... 77% 23.4M 0s\n  7950K .......... .......... .......... .......... .......... 78% 26.1M 0s\n  8000K .......... .......... .......... .......... .......... 78% 18.5M 0s\n  8050K .......... .......... .......... .......... .......... 79% 31.1M 0s\n  8100K .......... .......... .......... .......... .......... 79% 24.9M 0s\n  8150K .......... .......... .......... .......... .......... 80% 26.6M 0s\n  8200K .......... .......... .......... .......... .......... 80% 24.4M 0s\n  8250K .......... .......... .......... .......... .......... 81% 25.2M 0s\n  8300K .......... .......... .......... .......... .......... 81% 24.8M 0s\n  8350K .......... .......... .......... .......... .......... 82% 28.3M 0s\n  8400K .......... .......... .......... .......... .......... 82% 25.8M 0s\n  8450K .......... .......... .......... .......... .......... 83% 26.5M 0s\n  8500K .......... .......... .......... .......... .......... 83% 23.9M 0s\n  8550K .......... .......... .......... .......... .......... 83% 29.0M 0s\n  8600K .......... .......... .......... .......... .......... 84% 21.0M 0s\n  8650K .......... .......... .......... .......... .......... 84% 15.6M 0s\n  8700K .......... .......... .......... .......... .......... 85% 21.0M 0s\n  8750K .......... .......... .......... .......... .......... 85% 29.0M 0s\n  8800K .......... .......... .......... .......... .......... 86% 27.1M 0s\n  8850K .......... .......... .......... .......... .......... 86% 27.0M 0s\n  8900K .......... .......... .......... .......... .......... 87% 24.1M 0s\n  8950K .......... .......... .......... .......... .......... 87% 24.9M 0s\n  9000K .......... .......... .......... .......... .......... 88% 25.8M 0s\n  9050K .......... .......... .......... .......... .......... 88% 24.5M 0s\n  9100K .......... .......... .......... .......... .......... 89% 23.6M 0s\n  9150K .......... .......... .......... .......... .......... 89% 25.4M 0s\n  9200K .......... .......... .......... .......... .......... 90% 2.44M 0s\n  9250K .......... .......... .......... .......... .......... 90% 9.88M 0s\n  9300K .......... .......... .......... .......... .......... 91% 22.9M 0s\n  9350K .......... .......... .......... .......... .......... 91% 26.6M 0s\n  9400K .......... .......... .......... .......... .......... 92% 26.7M 0s\n  9450K .......... .......... .......... .......... .......... 92% 23.3M 0s\n  9500K .......... .......... .......... .......... .......... 93% 18.1M 0s\n  9550K .......... .......... .......... .......... .......... 93% 27.4M 0s\n  9600K .......... .......... .......... .......... .......... 94% 29.2M 0s\n  9650K .......... .......... .......... .......... .......... 94% 28.5M 0s\n  9700K .......... .......... .......... .......... .......... 95% 25.5M 0s\n  9750K .......... .......... .......... .......... .......... 95% 27.3M 0s\n  9800K .......... .......... .......... .......... .......... 96% 28.2M 0s\n  9850K .......... .......... .......... .......... .......... 96% 30.2M 0s\n  9900K .......... .......... .......... .......... .......... 97% 22.5M 0s\n  9950K .......... .......... .......... .......... .......... 97% 22.3M 0s\n 10000K .......... .......... .......... .......... .......... 98% 25.3M 0s\n 10050K .......... .......... .......... .......... .......... 98% 24.0M 0s\n 10100K .......... .......... .......... .......... .......... 99% 22.7M 0s\n 10150K .......... .......... .......... .......... .......... 99% 28.4M 0s\n 10200K .......... .......... .......... ........             100% 25.6M=1.0s\n\n2020-08-18 02:41:16 (10.2 MB/s) - ‘pubmed-sample.csv’ saved [10484510/10484510]\n\n</div>"]}}],"execution_count":114},{"cell_type":"code","source":["import pyspark.sql.functions as F\n\npubMedDF = spark.read\\\n                .option(\"header\", \"true\")\\\n                .csv(\"/dbfs/tmp/pubmed/pubmed-sample.csv\")\\\n                .filter(\"AB IS NOT null\")\\\n                .withColumnRenamed(\"AB\", \"text\")\\\n                .drop(\"TI\")\n\npubMedDF.show(truncate=50)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":478},"colab_type":"code","id":"ZyY6vL-gCOnV","outputId":"d5d6306b-0a71-402b-eda0-9fcbd41bdbb5"},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+--------------------------------------------------+\n                                              text|\n+--------------------------------------------------+\nThe human KCNJ9 (Kir 3.3, GIRK3) is a member of...|\nBACKGROUND: At present, it is one of the most i...|\nOBJECTIVE: To investigate the relationship betw...|\nCombined EEG/fMRI recording has been used to lo...|\nKohlschutter syndrome is a rare neurodegenerati...|\nStatistical analysis of neuroimages is commonly...|\nThe synthetic DOX-LNA conjugate was characteriz...|\nOur objective was to compare three different me...|\nWe conducted a phase II study to assess the eff...|\n&#34;Monomeric sarcosine oxidase (MSOX) is a flavoe...|\nWe presented the tachinid fly Exorista japonica...|\nThe literature dealing with the water conductin...|\nA novel approach to synthesize chitosan-O-isopr...|\nAn HPLC-ESI-MS-MS method has been developed for...|\nThe localizing and lateralizing values of eye a...|\nOBJECTIVE: To evaluate the effectiveness and ac...|\nFor the construction of new combinatorial libra...|\nWe report the results of a screen for genetic a...|\nIntraparenchymal pericatheter cyst is rarely re...|\nIt is known that patients with Klinefelter&#39;s sy...|\n+--------------------------------------------------+\nonly showing top 20 rows\n\n</div>"]}}],"execution_count":115},{"cell_type":"code","source":["pubMedDF.select('text').take(2)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":90},"colab_type":"code","id":"mY7ZBEpwESzO","outputId":"cf163eeb-cd78-4784-b480-a123f776306b"},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[82]: [Row(text=&#39;The human KCNJ9 (Kir 3.3, GIRK3) is a member of the G-protein-activated inwardly rectifying potassium (GIRK) channel family. Here we describe the genomicorganization of the KCNJ9 locus on chromosome 1q21-23 as a candidate gene forType II diabetes mellitus in the Pima Indian population. The gene spansapproximately 7.6 kb and contains one noncoding and two coding exons separated byapproximately 2.2 and approximately 2.6 kb introns, respectively. We identified14 single nucleotide polymorphisms (SNPs), including one that predicts aVal366Ala substitution, and an 8 base-pair (bp) insertion/deletion. Ourexpression studies revealed the presence of the transcript in various humantissues including pancreas, and two major insulin-responsive tissues: fat andskeletal muscle. The characterization of the KCNJ9 gene should facilitate furtherstudies on the function of the KCNJ9 protein and allow evaluation of thepotential role of the locus in Type II diabetes.&#39;),\n Row(text=&#39;BACKGROUND: At present, it is one of the most important issues for the treatment of breast cancer to develop the standard therapy for patients previously treated with anthracyclines and taxanes. With the objective of determining the usefulnessof vinorelbine monotherapy in patients with advanced or recurrent breast cancerafter standard therapy, we evaluated the efficacy and safety of vinorelbine inpatients previously treated with anthracyclines and taxanes. METHODS: Vinorelbinewas administered at a dose level of 25 mg/m(2) intravenously on days 1 and 8 of a3 week cycle. Patients were given three or more cycles in the absence of tumorprogression. A maximum of nine cycles were administered. RESULTS: The responserate in 50 evaluable patients was 20.0% (10 out of 50; 95% confidence interval,10.0-33.7%). Responders plus those who had minor response (MR) or no change (NC) accounted for 58.0% [10 partial responses (PRs) + one MR + 18 NCs out of 50]. TheKaplan-Meier estimate (50% point) of time to progression (TTP) was 115.0 days.The response rate in the visceral organs was 17.3% (nine PRs out of 52). Themajor toxicity was myelosuppression, which was reversible and did not requirediscontinuation of treatment. CONCLUSION: The results of this study show thatvinorelbine monotherapy is useful in patients with advanced or recurrent breastcancer previously exposed to both anthracyclines and taxanes.&#39;)]</div>"]}}],"execution_count":116},{"cell_type":"code","source":["%sh\nTMP=/FileStore/entities\nif [ ! -d \"$TMP\" ]; then\n    mkdir $TMP\nfi"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":117},{"cell_type":"code","source":["# write the target entities to txt file \n\nentities = ['KCNJ9', 'GIRK', 'diabetes mellitus', 'nucleotide polymorphisms']\ncontent = ''\nfor e in entities:\n  content = content + \"\\n\" + e\ndbutils.fs.put(\"dbfs:/tmp/pubmed/clinical_entities.txt\", content)\n\n\nentities = ['breast cancer', 'colon cancer', 'lung cancer', 'monotherapy', 'therapy']\ncontent=''\nfor e in entities:\n  content = content + \"\\n\" + e\ndbutils.fs.put(\"dbfs:/tmp/pubmed/cancer_entities.txt\", content)"],"metadata":{"colab":{},"colab_type":"code","id":"Hjr_4LOhELwW"},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">ExecutionError</span>                            Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-21558677108689&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      5</span> <span class=\"ansi-green-fg\">for</span> e <span class=\"ansi-green-fg\">in</span> entities<span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      6</span>   content <span class=\"ansi-blue-fg\">=</span> content <span class=\"ansi-blue-fg\">+</span> <span class=\"ansi-blue-fg\">&#34;\\n&#34;</span> <span class=\"ansi-blue-fg\">+</span> e\n<span class=\"ansi-green-fg\">----&gt; 7</span><span class=\"ansi-red-fg\"> </span>dbutils<span class=\"ansi-blue-fg\">.</span>fs<span class=\"ansi-blue-fg\">.</span>put<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;dbfs:/tmp/pubmed/clinical_entities.txt&#34;</span><span class=\"ansi-blue-fg\">,</span> content<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      8</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">      9</span> \n\n<span class=\"ansi-green-fg\">/local_disk0/tmp/1597717970634-0/dbutils.py</span> in <span class=\"ansi-cyan-fg\">f_with_exception_handling</span><span class=\"ansi-blue-fg\">(*args, **kwargs)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    312</span>                     exc<span class=\"ansi-blue-fg\">.</span>__context__ <span class=\"ansi-blue-fg\">=</span> <span class=\"ansi-green-fg\">None</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    313</span>                     exc<span class=\"ansi-blue-fg\">.</span>__cause__ <span class=\"ansi-blue-fg\">=</span> <span class=\"ansi-green-fg\">None</span>\n<span class=\"ansi-green-fg\">--&gt; 314</span><span class=\"ansi-red-fg\">                     </span><span class=\"ansi-green-fg\">raise</span> exc\n<span class=\"ansi-green-intense-fg ansi-bold\">    315</span>             <span class=\"ansi-green-fg\">return</span> f_with_exception_handling\n<span class=\"ansi-green-intense-fg ansi-bold\">    316</span> \n\n<span class=\"ansi-red-fg\">ExecutionError</span>: An error occurred while calling z:com.databricks.backend.daemon.dbutils.FSUtils.put.\n: org.apache.hadoop.fs.FileAlreadyExistsException: /oregon-prod/3418543878422327/tmp/pubmed/clinical_entities.txt already exists\n\tat com.databricks.s3a.S3AFileSystem.create(S3AFileSystem.java:629)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2$$anonfun$create$1$$anonfun$apply$10$$anonfun$apply$11.apply(DatabricksFileSystemV2.scala:544)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2$$anonfun$create$1$$anonfun$apply$10$$anonfun$apply$11.apply(DatabricksFileSystemV2.scala:541)\n\tat com.databricks.s3a.S3AExeceptionUtils$.convertAWSExceptionToJavaIOException(DatabricksStreamUtils.scala:119)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2$$anonfun$create$1$$anonfun$apply$10.apply(DatabricksFileSystemV2.scala:541)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2$$anonfun$create$1$$anonfun$apply$10.apply(DatabricksFileSystemV2.scala:541)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2$$anonfun$withUserContextRecorded$1.apply(DatabricksFileSystemV2.scala:936)\n\tat com.databricks.logging.UsageLogging$$anonfun$withAttributionContext$1.apply(UsageLogging.scala:238)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)\n\tat com.databricks.logging.UsageLogging$class.withAttributionContext(UsageLogging.scala:233)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.withAttributionContext(DatabricksFileSystemV2.scala:450)\n\tat com.databricks.logging.UsageLogging$class.withAttributionTags(UsageLogging.scala:275)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.withAttributionTags(DatabricksFileSystemV2.scala:450)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.withUserContextRecorded(DatabricksFileSystemV2.scala:909)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2$$anonfun$create$1.apply(DatabricksFileSystemV2.scala:540)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2$$anonfun$create$1.apply(DatabricksFileSystemV2.scala:540)\n\tat com.databricks.logging.UsageLogging$$anonfun$recordOperation$1.apply(UsageLogging.scala:428)\n\tat com.databricks.logging.UsageLogging$$anonfun$withAttributionContext$1.apply(UsageLogging.scala:238)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)\n\tat com.databricks.logging.UsageLogging$class.withAttributionContext(UsageLogging.scala:233)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.withAttributionContext(DatabricksFileSystemV2.scala:450)\n\tat com.databricks.logging.UsageLogging$class.withAttributionTags(UsageLogging.scala:275)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.withAttributionTags(DatabricksFileSystemV2.scala:450)\n\tat com.databricks.logging.UsageLogging$class.recordOperation(UsageLogging.scala:409)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.recordOperation(DatabricksFileSystemV2.scala:450)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.create(DatabricksFileSystemV2.scala:537)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystem.create(DatabricksFileSystem.scala:128)\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911)\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:892)\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:789)\n\tat com.databricks.backend.daemon.dbutils.FSUtils$$anonfun$put$1.apply(DBUtilsCore.scala:219)\n\tat com.databricks.backend.daemon.dbutils.FSUtils$$anonfun$put$1.apply(DBUtilsCore.scala:217)\n\tat com.databricks.backend.daemon.dbutils.FSUtils$.com$databricks$backend$daemon$dbutils$FSUtils$$withFsSafetyCheck(DBUtilsCore.scala:81)\n\tat com.databricks.backend.daemon.dbutils.FSUtils$.put(DBUtilsCore.scala:217)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.put(DBUtilsCore.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:295)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:251)\n\tat java.lang.Thread.run(Thread.java:748)\n</div>"]}}],"execution_count":118},{"cell_type":"code","source":["clinical_entity_extractor = TextMatcher() \\\n    .setInputCols([\"document\",'token'])\\\n    .setOutputCol(\"clinical_entities\")\\\n    .setEntities(\"dbfs:/tmp/pubmed/clinical_entities.txt\")\\\n    .setCaseSensitive(False)\\\n    .setEntityValue('clinical_entity')\n\ncancer_entity_extractor = TextMatcher() \\\n    .setInputCols([\"document\",'token'])\\\n    .setOutputCol(\"cancer_entities\")\\\n    .setEntities(\"dbfs:/tmp/entities/cancer_entities.txt\")\\\n    .setCaseSensitive(False)\\\n    .setEntityValue('cancer_entity')\n\n\nnlpPipeline = Pipeline(stages=[\n documentAssembler, \n tokenizer,\n clinical_entity_extractor,\n cancer_entity_extractor\n ])\n\nempty_df = spark.createDataFrame([['']]).toDF(\"text\")\n\npipelineModel = nlpPipeline.fit(empty_df)\n"],"metadata":{"colab":{},"colab_type":"code","id":"ExRo9nnCCgG3"},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":119},{"cell_type":"code","source":["result = pipelineModel.transform(pubMedDF.limit(30))"],"metadata":{"colab":{},"colab_type":"code","id":"gO0Uf2t1Ey1T"},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":120},{"cell_type":"code","source":["result.select('clinical_entities.result','cancer_entities.result').take(2)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":54},"colab_type":"code","id":"NctznXrSFb7Y","outputId":"78f2a84a-59fa-4175-ce3d-b86d270850cc"},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[108]: [Row(result=[&#39;KCNJ9&#39;, &#39;GIRK&#39;, &#39;KCNJ9&#39;, &#39;diabetes mellitus&#39;, &#39;nucleotide polymorphisms&#39;, &#39;KCNJ9&#39;, &#39;KCNJ9&#39;], result=[]),\n Row(result=[], result=[&#39;breast cancer&#39;, &#39;therapy&#39;, &#39;monotherapy&#39;, &#39;therapy&#39;, &#39;monotherapy&#39;])]</div>"]}}],"execution_count":121},{"cell_type":"code","source":["result.select('clinical_entities','cancer_entities').take(2)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[109]: [Row(clinical_entities=[Row(annotatorType=&#39;chunk&#39;, begin=10, end=14, result=&#39;KCNJ9&#39;, metadata={&#39;sentence&#39;: &#39;0&#39;, &#39;chunk&#39;: &#39;0&#39;, &#39;entity&#39;: &#39;clinical_entity&#39;}, embeddings=[]), Row(annotatorType=&#39;chunk&#39;, begin=103, end=106, result=&#39;GIRK&#39;, metadata={&#39;sentence&#39;: &#39;0&#39;, &#39;chunk&#39;: &#39;1&#39;, &#39;entity&#39;: &#39;clinical_entity&#39;}, embeddings=[]), Row(annotatorType=&#39;chunk&#39;, begin=173, end=177, result=&#39;KCNJ9&#39;, metadata={&#39;sentence&#39;: &#39;0&#39;, &#39;chunk&#39;: &#39;2&#39;, &#39;entity&#39;: &#39;clinical_entity&#39;}, embeddings=[]), Row(annotatorType=&#39;chunk&#39;, begin=238, end=254, result=&#39;diabetes mellitus&#39;, metadata={&#39;sentence&#39;: &#39;0&#39;, &#39;chunk&#39;: &#39;3&#39;, &#39;entity&#39;: &#39;clinical_entity&#39;}, embeddings=[]), Row(annotatorType=&#39;chunk&#39;, begin=471, end=494, result=&#39;nucleotide polymorphisms&#39;, metadata={&#39;sentence&#39;: &#39;0&#39;, &#39;chunk&#39;: &#39;4&#39;, &#39;entity&#39;: &#39;clinical_entity&#39;}, embeddings=[]), Row(annotatorType=&#39;chunk&#39;, begin=801, end=805, result=&#39;KCNJ9&#39;, metadata={&#39;sentence&#39;: &#39;0&#39;, &#39;chunk&#39;: &#39;5&#39;, &#39;entity&#39;: &#39;clinical_entity&#39;}, embeddings=[]), Row(annotatorType=&#39;chunk&#39;, begin=868, end=872, result=&#39;KCNJ9&#39;, metadata={&#39;sentence&#39;: &#39;0&#39;, &#39;chunk&#39;: &#39;6&#39;, &#39;entity&#39;: &#39;clinical_entity&#39;}, embeddings=[])], cancer_entities=[]),\n Row(clinical_entities=[], cancer_entities=[Row(annotatorType=&#39;chunk&#39;, begin=84, end=96, result=&#39;breast cancer&#39;, metadata={&#39;sentence&#39;: &#39;0&#39;, &#39;chunk&#39;: &#39;0&#39;, &#39;entity&#39;: &#39;cancer_entity&#39;}, embeddings=[]), Row(annotatorType=&#39;chunk&#39;, begin=122, end=128, result=&#39;therapy&#39;, metadata={&#39;sentence&#39;: &#39;0&#39;, &#39;chunk&#39;: &#39;1&#39;, &#39;entity&#39;: &#39;cancer_entity&#39;}, embeddings=[]), Row(annotatorType=&#39;chunk&#39;, begin=258, end=268, result=&#39;monotherapy&#39;, metadata={&#39;sentence&#39;: &#39;0&#39;, &#39;chunk&#39;: &#39;2&#39;, &#39;entity&#39;: &#39;cancer_entity&#39;}, embeddings=[]), Row(annotatorType=&#39;chunk&#39;, begin=337, end=343, result=&#39;therapy&#39;, metadata={&#39;sentence&#39;: &#39;0&#39;, &#39;chunk&#39;: &#39;3&#39;, &#39;entity&#39;: &#39;cancer_entity&#39;}, embeddings=[]), Row(annotatorType=&#39;chunk&#39;, begin=1279, end=1289, result=&#39;monotherapy&#39;, metadata={&#39;sentence&#39;: &#39;0&#39;, &#39;chunk&#39;: &#39;4&#39;, &#39;entity&#39;: &#39;cancer_entity&#39;}, embeddings=[])])]</div>"]}}],"execution_count":122},{"cell_type":"code","source":["result_df = result.select(F.explode(F.arrays_zip('clinical_entities.result', 'clinical_entities.begin',  'clinical_entities.end')).alias(\"cols\")) \\\n.select(F.expr(\"cols['0']\").alias(\"clinical_entities\"),\n        F.expr(\"cols['1']\").alias(\"begin\"),\n        F.expr(\"cols['2']\").alias(\"end\")).toPandas()\nresult_df.head(10)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":257},"colab_type":"code","id":"-bVEDIblFoL6","outputId":"424bde7d-35aa-4693-dfbf-2cf7b5e133db"},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>clinical_entities</th>\n      <th>begin</th>\n      <th>end</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>KCNJ9</td>\n      <td>10</td>\n      <td>14</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>GIRK</td>\n      <td>103</td>\n      <td>106</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>KCNJ9</td>\n      <td>173</td>\n      <td>177</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>diabetes mellitus</td>\n      <td>238</td>\n      <td>254</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>nucleotide polymorphisms</td>\n      <td>471</td>\n      <td>494</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>KCNJ9</td>\n      <td>801</td>\n      <td>805</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>KCNJ9</td>\n      <td>868</td>\n      <td>872</td>\n    </tr>\n  </tbody>\n</table>\n</div>"]}}],"execution_count":123},{"cell_type":"code","source":["result_df = result.select(F.explode(F.arrays_zip('cancer_entities.result', 'cancer_entities.begin',  'cancer_entities.end')).alias(\"cols\")) \\\n.select(F.expr(\"cols['0']\").alias(\"cancer_entities\"),\n        F.expr(\"cols['1']\").alias(\"begin\"),\n        F.expr(\"cols['2']\").alias(\"end\")).toPandas()\n\nresult_df.head(10)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>cancer_entities</th>\n      <th>begin</th>\n      <th>end</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>therapy</td>\n      <td>545</td>\n      <td>551</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>therapy</td>\n      <td>66</td>\n      <td>72</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>therapy</td>\n      <td>510</td>\n      <td>516</td>\n    </tr>\n  </tbody>\n</table>\n</div>"]}}],"execution_count":124},{"cell_type":"markdown","source":["## RegexMatcher"],"metadata":{}},{"cell_type":"code","source":["rules = '''\nrenal\\s\\w+, started with 'renal'\ncardiac\\s\\w+, started with 'cardiac'\n\\w*ly\\b, ending with 'ly'\n\\S*\\d+\\S*, match any word that contains numbers\n(\\d+).?(\\d*)\\s*(mg|ml|g), match medication metrics\n'''\n\ndbutils.fs.put(\"dbfs:/tmp/pubmed/regex_rules.txt\", rules)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Wrote 195 bytes.\nOut[115]: True</div>"]}}],"execution_count":126},{"cell_type":"code","source":["import os\n\ndocumentAssembler = DocumentAssembler()\\\n.setInputCol(\"text\")\\\n.setOutputCol(\"document\")\n\nregex_matcher = RegexMatcher()\\\n    .setInputCols('document')\\\n    .setStrategy(\"MATCH_ALL\")\\\n    .setOutputCol(\"regex_matches\")\\\n    .setExternalRules(path='dbfs:/tmp/pubmed/regex_rules.txt', delimiter=',')\n    \n\nnlpPipeline = Pipeline(stages=[\n documentAssembler, \n regex_matcher\n ])\n\nempty_df = spark.createDataFrame([['']]).toDF(\"text\")\n\npipelineModel = nlpPipeline.fit(empty_df)\n\nmatch_df = pipelineModel.transform(pubMedDF)\n\nmatch_df.select('regex_matches.result').take(3)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[116]: [Row(result=[&#39;inwardly&#39;, &#39;family&#39;, &#39;spansapproximately&#39;, &#39;byapproximately&#39;, &#39;approximately&#39;, &#39;respectively&#39;, &#39;poly&#39;, &#39;KCNJ9&#39;, &#39;3.3,&#39;, &#39;GIRK3)&#39;, &#39;KCNJ9&#39;, &#39;1q21-23&#39;, &#39;7.6&#39;, &#39;2.2&#39;, &#39;2.6&#39;, &#39;identified14&#39;, &#39;aVal366Ala&#39;, &#39;8&#39;, &#39;KCNJ9&#39;, &#39;KCNJ9&#39;, &#39;9 g&#39;]),\n Row(result=[&#39;previously&#39;, &#39;previously&#39;, &#39;intravenously&#39;, &#39;previously&#39;, &#39;25&#39;, &#39;mg/m(2)&#39;, &#39;1&#39;, &#39;8&#39;, &#39;a3&#39;, &#39;50&#39;, &#39;20.0%&#39;, &#39;(10&#39;, &#39;50;&#39;, &#39;95%&#39;, &#39;interval,10.0-33.7%).&#39;, &#39;58.0%&#39;, &#39;[10&#39;, &#39;18&#39;, &#39;50].&#39;, &#39;(50%&#39;, &#39;115.0&#39;, &#39;17.3%&#39;, &#39;52).&#39;, &#39;25 mg&#39;]),\n Row(result=[&#39;renal failure&#39;, &#39;cardiac surgery&#39;, &#39;cardiac surgery&#39;, &#39;cardiac surgical&#39;, &#39;early&#39;, &#39;statistically&#39;, &#39;analy&#39;, &#39;1995&#39;, &#39;2005&#39;, &#39;=9796).&#39;, &#39;2.9&#39;, &#39;11years).&#39;, &#39;11.3%&#39;, &#39;1105),&#39;, &#39;7.2%&#39;, &#39;30%&#39;, &#39;0.0001),&#39;, &#39;1.55,95%&#39;, &#39;1.42-1.70,&#39;, &#39;0.0001).&#39;])]</div>"]}}],"execution_count":127},{"cell_type":"code","source":["match_df.select('text','regex_matches.result')\\\n.toDF('text','matches').filter(F.size('matches')>1)\\\n.show(truncate=50)\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+--------------------------------------------------+--------------------------------------------------+\n                                              text|                                           matches|\n+--------------------------------------------------+--------------------------------------------------+\nThe human KCNJ9 (Kir 3.3, GIRK3) is a member of...|[inwardly, family, spansapproximately, byapprox...|\nBACKGROUND: At present, it is one of the most i...|[previously, previously, intravenously, previou...|\nOBJECTIVE: To investigate the relationship betw...|[renal failure, cardiac surgery, cardiac surger...|\nCombined EEG/fMRI recording has been used to lo...|[normally, significantly, effectively, analy, o...|\nStatistical analysis of neuroimages is commonly...|[analy, commonly, overly, normally, thatsuccess...|\nThe synthetic DOX-LNA conjugate was characteriz...|                         [wasanaly, substantially]|\nOur objective was to compare three different me...|[daily, only, Conversely, Hourly, hourly, Hourl...|\nWe conducted a phase II study to assess the eff...|[analy, respectively, generally, 5-fluorouracil...|\n&#34;Monomeric sarcosine oxidase (MSOX) is a flavoe...|[cataly, methylgly, gly, ethylgly, dimethylgly,...|\nWe presented the tachinid fly Exorista japonica...|                         [fly, fly, fly, fly, fly]|\nThe literature dealing with the water conductin...|           [generally, mathematically, especially]|\nA novel approach to synthesize chitosan-O-isopr...|[efficiently, poly, chitosan-O-isopropyl-5&#39;-O-d...|\nAn HPLC-ESI-MS-MS method has been developed for...|[chromatographically, respectively, successfull...|\nThe localizing and lateralizing values of eye a...|                                    [early, early]|\nOBJECTIVE: To evaluate the effectiveness and ac...|[weekly, respectively, theanaly, 2006, 2007,, 2...|\nWe report the results of a screen for genetic a...|[poly, threepoly, significantly, analy, actuall...|\nIntraparenchymal pericatheter cyst is rarely re...|              [rarely, possibly, unusually, Early]|\nPURPOSE: To compare the effectiveness, potentia...|[analy, comparatively, wassignificantly, respec...|\nWe have demonstrated a new type of all-optical ...|[approximately, fully, approximately, approxima...|\nPhysalis peruviana (PP) is a widely used medici...|[widely, (20,, 40,, 60,, 80, 95%, 100, 95%, (82...|\n+--------------------------------------------------+--------------------------------------------------+\nonly showing top 20 rows\n\n</div>"]}}],"execution_count":128},{"cell_type":"markdown","source":["## Text Cleaning with UDF"],"metadata":{}},{"cell_type":"code","source":["text = '<h1 style=\"color: #5e9ca0;\">Have a great <span  style=\"color: #2b2301;\">birth</span> day!</h1>'\n\ntext_df = spark.createDataFrame([[text]]).toDF(\"text\")\n\nimport re\nfrom pyspark.sql.functions import udf\nfrom pyspark.sql.types import StringType, IntegerType\n\nclean_text = lambda s: re.sub(r'<[^>]*>', '', s)\n\ntext_df.withColumn('cleaned', udf(clean_text, StringType())('text')).select('text','cleaned').show(truncate= False)\n\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----------------------------------------------------------------------------------------------+-----------------------+\ntext                                                                                          |cleaned                |\n+----------------------------------------------------------------------------------------------+-----------------------+\n&lt;h1 style=&#34;color: #5e9ca0;&#34;&gt;Have a great &lt;span  style=&#34;color: #2b2301;&#34;&gt;birth&lt;/span&gt; day!&lt;/h1&gt;|Have a great birth day!|\n+----------------------------------------------------------------------------------------------+-----------------------+\n\n</div>"]}}],"execution_count":130},{"cell_type":"code","source":["find_not_alnum_count = lambda s: len([i for i in s if not i.isalnum() and i!=' '])\n\nfind_not_alnum_count(\"it's your birth day!\")"],"metadata":{},"outputs":[],"execution_count":131},{"cell_type":"code","source":["find_not_alnum_count = lambda s: len([i for i in s if not i.isalnum() and i!=' '])\n\nfind_not_alnum_count(\"it's your birth day!\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[119]: 2</div>"]}}],"execution_count":132},{"cell_type":"code","source":["text = '<h1 style=\"color: #5e9ca0;\">Have a great <span  style=\"color: #2b2301;\">birth</span> day!</h1>'\n\nfind_not_alnum_count(text)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[120]: 23</div>"]}}],"execution_count":133},{"cell_type":"code","source":["text_df.withColumn('cleaned', udf(find_not_alnum_count, IntegerType())('text')).select('text','cleaned').show(truncate= False)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----------------------------------------------------------------------------------------------+-------+\ntext                                                                                          |cleaned|\n+----------------------------------------------------------------------------------------------+-------+\n&lt;h1 style=&#34;color: #5e9ca0;&#34;&gt;Have a great &lt;span  style=&#34;color: #2b2301;&#34;&gt;birth&lt;/span&gt; day!&lt;/h1&gt;|23     |\n+----------------------------------------------------------------------------------------------+-------+\n\n</div>"]}}],"execution_count":134},{"cell_type":"markdown","source":["## Finisher"],"metadata":{}},{"cell_type":"markdown","source":["***Finisher:*** Once we have our NLP pipeline ready to go, we might want to use our annotation results somewhere else where it is easy to use. The Finisher outputs annotation(s) values into a string.\n\nIf we just want the desired output column in the final dataframe, we can use Finisher to drop previous stages in the final output and gte the `result` from the process.\n\nThis is very handy when you want to use the output from Spark NLP annotator as an input to another Spark ML transformer.\n\nSettable parameters are:\n\n`setInputCols()`\n\n`setOutputCols()`\n\n`setCleanAnnotations(True)` -> Whether to remove intermediate annotations\n\n`setValueSplitSymbol(“#”)` -> split values within an annotation character\n\n`setAnnotationSplitSymbol(“@”)` -> split values between annotations character\n\n`setIncludeMetadata(False)` -> Whether to include metadata keys. Sometimes useful in some annotations.\n\n`setOutputAsArray(False)` -> Whether to output as Array. Useful as input for other Spark transformers."],"metadata":{}},{"cell_type":"code","source":["finisher = Finisher() \\\n    .setInputCols([\"regex_matches\"]) \\\n    .setIncludeMetadata(False) # set to False to remove metadata\n\nnlpPipeline = Pipeline(stages=[\n documentAssembler, \n regex_matcher,\n finisher\n ])\n\nempty_df = spark.createDataFrame([['']]).toDF(\"text\")\n\npipelineModel = nlpPipeline.fit(empty_df)\n\nmatch_df = pipelineModel.transform(pubMedDF)\n\nmatch_df.show(truncate = 50)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+--------------------------------------------------+--------------------------------------------------+\n                                              text|                            finished_regex_matches|\n+--------------------------------------------------+--------------------------------------------------+\nThe human KCNJ9 (Kir 3.3, GIRK3) is a member of...|[inwardly, family, spansapproximately, byapprox...|\nBACKGROUND: At present, it is one of the most i...|[previously, previously, intravenously, previou...|\nOBJECTIVE: To investigate the relationship betw...|[renal failure, cardiac surgery, cardiac surger...|\nCombined EEG/fMRI recording has been used to lo...|[normally, significantly, effectively, analy, o...|\nKohlschutter syndrome is a rare neurodegenerati...|                                          [family]|\nStatistical analysis of neuroimages is commonly...|[analy, commonly, overly, normally, thatsuccess...|\nThe synthetic DOX-LNA conjugate was characteriz...|                         [wasanaly, substantially]|\nOur objective was to compare three different me...|[daily, only, Conversely, Hourly, hourly, Hourl...|\nWe conducted a phase II study to assess the eff...|[analy, respectively, generally, 5-fluorouracil...|\n&#34;Monomeric sarcosine oxidase (MSOX) is a flavoe...|[cataly, methylgly, gly, ethylgly, dimethylgly,...|\nWe presented the tachinid fly Exorista japonica...|                         [fly, fly, fly, fly, fly]|\nThe literature dealing with the water conductin...|           [generally, mathematically, especially]|\nA novel approach to synthesize chitosan-O-isopr...|[efficiently, poly, chitosan-O-isopropyl-5&#39;-O-d...|\nAn HPLC-ESI-MS-MS method has been developed for...|[chromatographically, respectively, successfull...|\nThe localizing and lateralizing values of eye a...|                                    [early, early]|\nOBJECTIVE: To evaluate the effectiveness and ac...|[weekly, respectively, theanaly, 2006, 2007,, 2...|\nFor the construction of new combinatorial libra...|                                           [newly]|\nWe report the results of a screen for genetic a...|[poly, threepoly, significantly, analy, actuall...|\nIntraparenchymal pericatheter cyst is rarely re...|              [rarely, possibly, unusually, Early]|\nIt is known that patients with Klinefelter&#39;s sy...|                                                []|\n+--------------------------------------------------+--------------------------------------------------+\nonly showing top 20 rows\n\n</div>"]}}],"execution_count":137},{"cell_type":"code","source":["match_df.printSchema()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">root\n-- text: string (nullable = true)\n-- finished_regex_matches: array (nullable = true)\n    |-- element: string (containsNull = true)\n\n</div>"]}}],"execution_count":138},{"cell_type":"code","source":["match_df.filter(F.size('finished_regex_matches')>1).show(truncate = 50)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+--------------------------------------------------+--------------------------------------------------+\n                                              text|                            finished_regex_matches|\n+--------------------------------------------------+--------------------------------------------------+\nThe human KCNJ9 (Kir 3.3, GIRK3) is a member of...|[inwardly, family, spansapproximately, byapprox...|\nBACKGROUND: At present, it is one of the most i...|[previously, previously, intravenously, previou...|\nOBJECTIVE: To investigate the relationship betw...|[renal failure, cardiac surgery, cardiac surger...|\nCombined EEG/fMRI recording has been used to lo...|[normally, significantly, effectively, analy, o...|\nStatistical analysis of neuroimages is commonly...|[analy, commonly, overly, normally, thatsuccess...|\nThe synthetic DOX-LNA conjugate was characteriz...|                         [wasanaly, substantially]|\nOur objective was to compare three different me...|[daily, only, Conversely, Hourly, hourly, Hourl...|\nWe conducted a phase II study to assess the eff...|[analy, respectively, generally, 5-fluorouracil...|\n&#34;Monomeric sarcosine oxidase (MSOX) is a flavoe...|[cataly, methylgly, gly, ethylgly, dimethylgly,...|\nWe presented the tachinid fly Exorista japonica...|                         [fly, fly, fly, fly, fly]|\nThe literature dealing with the water conductin...|           [generally, mathematically, especially]|\nA novel approach to synthesize chitosan-O-isopr...|[efficiently, poly, chitosan-O-isopropyl-5&#39;-O-d...|\nAn HPLC-ESI-MS-MS method has been developed for...|[chromatographically, respectively, successfull...|\nThe localizing and lateralizing values of eye a...|                                    [early, early]|\nOBJECTIVE: To evaluate the effectiveness and ac...|[weekly, respectively, theanaly, 2006, 2007,, 2...|\nWe report the results of a screen for genetic a...|[poly, threepoly, significantly, analy, actuall...|\nIntraparenchymal pericatheter cyst is rarely re...|              [rarely, possibly, unusually, Early]|\nPURPOSE: To compare the effectiveness, potentia...|[analy, comparatively, wassignificantly, respec...|\nWe have demonstrated a new type of all-optical ...|[approximately, fully, approximately, approxima...|\nPhysalis peruviana (PP) is a widely used medici...|[widely, (20,, 40,, 60,, 80, 95%, 100, 95%, (82...|\n+--------------------------------------------------+--------------------------------------------------+\nonly showing top 20 rows\n\n</div>"]}}],"execution_count":139},{"cell_type":"markdown","source":["## LightPipeline"],"metadata":{"colab_type":"text","id":"RBeh9Yv44elz"}},{"cell_type":"markdown","source":["LightPipelines are Spark NLP specific Pipelines, equivalent to Spark ML Pipeline, but meant to deal with smaller amounts of data. They’re useful working with small datasets, debugging results, or when running either training or prediction from an API that serves one-off requests.\n\nSpark NLP LightPipelines are Spark ML pipelines converted into a single machine but the multi-threaded task, becoming more than 10x times faster for smaller amounts of data (small is relative, but 50k sentences are roughly a good maximum). To use them, we simply plug in a trained (fitted) pipeline and then annotate a plain text. We don't even need to convert the input text to DataFrame in order to feed it into a pipeline that's accepting DataFrame as an input in the first place. This feature would be quite useful when it comes to getting a prediction for a few lines of text from a trained ML model.\n\n **It is nearly 20x faster than using Spark ML Pipeline**\n\n`LightPipeline(someTrainedPipeline).annotate(someStringOrArray)`"],"metadata":{"colab_type":"text","id":"eaANY-Bg4paN"}},{"cell_type":"code","source":["documentAssembler = DocumentAssembler()\\\n.setInputCol(\"text\")\\\n.setOutputCol(\"document\")\n\ntokenizer = Tokenizer() \\\n    .setInputCols([\"document\"]) \\\n    .setOutputCol(\"token\")\n\nstemmer = Stemmer() \\\n    .setInputCols([\"token\"]) \\\n    .setOutputCol(\"stem\")\n\nnlpPipeline = Pipeline(stages=[\n documentAssembler, \n tokenizer,\n stemmer,\n lemmatizer\n ])\n\nempty_df = spark.createDataFrame([['']]).toDF(\"text\")\n\npipelineModel = nlpPipeline.fit(empty_df)\n\npipelineModel.transform(spark_df).show()\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+--------------------+--------------------+--------------------+--------------------+--------------------+\n                text|            document|               token|                stem|               lemma|\n+--------------------+--------------------+--------------------+--------------------+--------------------+\nPeter is a very g...|[[document, 0, 27...|[[token, 0, 4, Pe...|[[token, 0, 4, pe...|[[token, 0, 4, Pe...|\nMy life in Russia...|[[document, 0, 37...|[[token, 0, 1, My...|[[token, 0, 1, my...|[[token, 0, 1, My...|\nJohn and Peter ar...|[[document, 0, 76...|[[token, 0, 3, Jo...|[[token, 0, 3, jo...|[[token, 0, 3, Jo...|\nLucas Nogal Dunbe...|[[document, 0, 67...|[[token, 0, 4, Lu...|[[token, 0, 4, lu...|[[token, 0, 4, Lu...|\nEurope is very cu...|[[document, 0, 68...|[[token, 0, 5, Eu...|[[token, 0, 5, eu...|[[token, 0, 5, Eu...|\n+--------------------+--------------------+--------------------+--------------------+--------------------+\n\n</div>"]}}],"execution_count":142},{"cell_type":"code","source":["from sparknlp.base import LightPipeline\n\nlight_model = LightPipeline(pipelineModel)\n\nlight_result = light_model.annotate(\"John and Peter are brothers. However they don't support each other that much.\")"],"metadata":{"colab":{},"colab_type":"code","id":"kilQL1ps4kXh"},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":143},{"cell_type":"code","source":["light_result.keys()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":34},"colab_type":"code","id":"Sw9Z5q_H49M_","outputId":"96e1267a-237c-425a-c8ab-004e6b395296"},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[127]: dict_keys([&#39;document&#39;, &#39;token&#39;, &#39;stem&#39;, &#39;lemma&#39;])</div>"]}}],"execution_count":144},{"cell_type":"code","source":["list(zip(light_result['token'], light_result['stem'], light_result['lemma']))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":283},"colab_type":"code","id":"3x4u11yQ5CxR","outputId":"d954dd78-7d5d-4452-9573-019c601cce8e"},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[128]: [(&#39;John&#39;, &#39;john&#39;, &#39;John&#39;),\n (&#39;and&#39;, &#39;and&#39;, &#39;and&#39;),\n (&#39;Peter&#39;, &#39;peter&#39;, &#39;Peter&#39;),\n (&#39;are&#39;, &#39;ar&#39;, &#39;be&#39;),\n (&#39;brothers&#39;, &#39;brother&#39;, &#39;brother&#39;),\n (&#39;.&#39;, &#39;.&#39;, &#39;.&#39;),\n (&#39;However&#39;, &#39;howev&#39;, &#39;However&#39;),\n (&#39;they&#39;, &#39;thei&#39;, &#39;they&#39;),\n (&#34;don&#39;t&#34;, &#34;don&#39;t&#34;, &#34;don&#39;t&#34;),\n (&#39;support&#39;, &#39;support&#39;, &#39;support&#39;),\n (&#39;each&#39;, &#39;each&#39;, &#39;each&#39;),\n (&#39;other&#39;, &#39;other&#39;, &#39;other&#39;),\n (&#39;that&#39;, &#39;that&#39;, &#39;that&#39;),\n (&#39;much&#39;, &#39;much&#39;, &#39;much&#39;),\n (&#39;.&#39;, &#39;.&#39;, &#39;.&#39;)]</div>"]}}],"execution_count":145},{"cell_type":"code","source":["light_result = light_model.fullAnnotate(\"John and Peter are brothers. However they don't support each other that much.\")"],"metadata":{"colab":{},"colab_type":"code","id":"cOe6aYzn5NXG"},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":146},{"cell_type":"code","source":["light_result"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":854},"colab_type":"code","id":"nObeWNkt55pS","outputId":"3d621d85-a624-4b9b-c837-844f4f9cda80"},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[130]: [{&#39;document&#39;: [Annotation(document, 0, 76, John and Peter are brothers. However they don&#39;t support each other that much., {})],\n  &#39;token&#39;: [Annotation(token, 0, 3, John, {&#39;sentence&#39;: &#39;0&#39;}),\n   Annotation(token, 5, 7, and, {&#39;sentence&#39;: &#39;0&#39;}),\n   Annotation(token, 9, 13, Peter, {&#39;sentence&#39;: &#39;0&#39;}),\n   Annotation(token, 15, 17, are, {&#39;sentence&#39;: &#39;0&#39;}),\n   Annotation(token, 19, 26, brothers, {&#39;sentence&#39;: &#39;0&#39;}),\n   Annotation(token, 27, 27, ., {&#39;sentence&#39;: &#39;0&#39;}),\n   Annotation(token, 29, 35, However, {&#39;sentence&#39;: &#39;0&#39;}),\n   Annotation(token, 37, 40, they, {&#39;sentence&#39;: &#39;0&#39;}),\n   Annotation(token, 42, 46, don&#39;t, {&#39;sentence&#39;: &#39;0&#39;}),\n   Annotation(token, 48, 54, support, {&#39;sentence&#39;: &#39;0&#39;}),\n   Annotation(token, 56, 59, each, {&#39;sentence&#39;: &#39;0&#39;}),\n   Annotation(token, 61, 65, other, {&#39;sentence&#39;: &#39;0&#39;}),\n   Annotation(token, 67, 70, that, {&#39;sentence&#39;: &#39;0&#39;}),\n   Annotation(token, 72, 75, much, {&#39;sentence&#39;: &#39;0&#39;}),\n   Annotation(token, 76, 76, ., {&#39;sentence&#39;: &#39;0&#39;})],\n  &#39;stem&#39;: [Annotation(token, 0, 3, john, {&#39;sentence&#39;: &#39;0&#39;}),\n   Annotation(token, 5, 7, and, {&#39;sentence&#39;: &#39;0&#39;}),\n   Annotation(token, 9, 13, peter, {&#39;sentence&#39;: &#39;0&#39;}),\n   Annotation(token, 15, 17, ar, {&#39;sentence&#39;: &#39;0&#39;}),\n   Annotation(token, 19, 26, brother, {&#39;sentence&#39;: &#39;0&#39;}),\n   Annotation(token, 27, 27, ., {&#39;sentence&#39;: &#39;0&#39;}),\n   Annotation(token, 29, 35, howev, {&#39;sentence&#39;: &#39;0&#39;}),\n   Annotation(token, 37, 40, thei, {&#39;sentence&#39;: &#39;0&#39;}),\n   Annotation(token, 42, 46, don&#39;t, {&#39;sentence&#39;: &#39;0&#39;}),\n   Annotation(token, 48, 54, support, {&#39;sentence&#39;: &#39;0&#39;}),\n   Annotation(token, 56, 59, each, {&#39;sentence&#39;: &#39;0&#39;}),\n   Annotation(token, 61, 65, other, {&#39;sentence&#39;: &#39;0&#39;}),\n   Annotation(token, 67, 70, that, {&#39;sentence&#39;: &#39;0&#39;}),\n   Annotation(token, 72, 75, much, {&#39;sentence&#39;: &#39;0&#39;}),\n   Annotation(token, 76, 76, ., {&#39;sentence&#39;: &#39;0&#39;})],\n  &#39;lemma&#39;: [Annotation(token, 0, 3, John, {&#39;sentence&#39;: &#39;0&#39;}),\n   Annotation(token, 5, 7, and, {&#39;sentence&#39;: &#39;0&#39;}),\n   Annotation(token, 9, 13, Peter, {&#39;sentence&#39;: &#39;0&#39;}),\n   Annotation(token, 15, 17, be, {&#39;sentence&#39;: &#39;0&#39;}),\n   Annotation(token, 19, 26, brother, {&#39;sentence&#39;: &#39;0&#39;}),\n   Annotation(token, 27, 27, ., {&#39;sentence&#39;: &#39;0&#39;}),\n   Annotation(token, 29, 35, However, {&#39;sentence&#39;: &#39;0&#39;}),\n   Annotation(token, 37, 40, they, {&#39;sentence&#39;: &#39;0&#39;}),\n   Annotation(token, 42, 46, don&#39;t, {&#39;sentence&#39;: &#39;0&#39;}),\n   Annotation(token, 48, 54, support, {&#39;sentence&#39;: &#39;0&#39;}),\n   Annotation(token, 56, 59, each, {&#39;sentence&#39;: &#39;0&#39;}),\n   Annotation(token, 61, 65, other, {&#39;sentence&#39;: &#39;0&#39;}),\n   Annotation(token, 67, 70, that, {&#39;sentence&#39;: &#39;0&#39;}),\n   Annotation(token, 72, 75, much, {&#39;sentence&#39;: &#39;0&#39;}),\n   Annotation(token, 76, 76, ., {&#39;sentence&#39;: &#39;0&#39;})]}]</div>"]}}],"execution_count":147},{"cell_type":"code","source":["text_list= [\"How did serfdom develop in and then leave Russia ?\",\n\"There will be some exciting breakthroughs in NLP this year.\"]\n\nlight_model.annotate(text_list)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"colab_type":"code","id":"uEgemHyB57LJ","outputId":"e861ed2c-1383-4105-a223-e33d52a1f1aa"},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[131]: [{&#39;document&#39;: [&#39;How did serfdom develop in and then leave Russia ?&#39;],\n  &#39;token&#39;: [&#39;How&#39;,\n   &#39;did&#39;,\n   &#39;serfdom&#39;,\n   &#39;develop&#39;,\n   &#39;in&#39;,\n   &#39;and&#39;,\n   &#39;then&#39;,\n   &#39;leave&#39;,\n   &#39;Russia&#39;,\n   &#39;?&#39;],\n  &#39;stem&#39;: [&#39;how&#39;,\n   &#39;did&#39;,\n   &#39;serfdom&#39;,\n   &#39;develop&#39;,\n   &#39;in&#39;,\n   &#39;and&#39;,\n   &#39;then&#39;,\n   &#39;leav&#39;,\n   &#39;russia&#39;,\n   &#39;?&#39;],\n  &#39;lemma&#39;: [&#39;How&#39;,\n   &#39;do&#39;,\n   &#39;serfdom&#39;,\n   &#39;develop&#39;,\n   &#39;in&#39;,\n   &#39;and&#39;,\n   &#39;then&#39;,\n   &#39;leave&#39;,\n   &#39;Russia&#39;,\n   &#39;?&#39;]},\n {&#39;document&#39;: [&#39;There will be some exciting breakthroughs in NLP this year.&#39;],\n  &#39;token&#39;: [&#39;There&#39;,\n   &#39;will&#39;,\n   &#39;be&#39;,\n   &#39;some&#39;,\n   &#39;exciting&#39;,\n   &#39;breakthroughs&#39;,\n   &#39;in&#39;,\n   &#39;NLP&#39;,\n   &#39;this&#39;,\n   &#39;year&#39;,\n   &#39;.&#39;],\n  &#39;stem&#39;: [&#39;there&#39;,\n   &#39;will&#39;,\n   &#39;be&#39;,\n   &#39;some&#39;,\n   &#39;excit&#39;,\n   &#39;breakthrough&#39;,\n   &#39;in&#39;,\n   &#39;nlp&#39;,\n   &#39;thi&#39;,\n   &#39;year&#39;,\n   &#39;.&#39;],\n  &#39;lemma&#39;: [&#39;There&#39;,\n   &#39;will&#39;,\n   &#39;be&#39;,\n   &#39;some&#39;,\n   &#39;exciting&#39;,\n   &#39;breakthrough&#39;,\n   &#39;in&#39;,\n   &#39;NLP&#39;,\n   &#39;this&#39;,\n   &#39;year&#39;,\n   &#39;.&#39;]}]</div>"]}}],"execution_count":148},{"cell_type":"code","source":["## important note: When you use Finisher in your pipeline, regardless of setting cleanAnnotations to False or True, LigtPipeline will only return the finished columns."],"metadata":{"colab":{},"colab_type":"code","id":"7PsXc1Ad6OH5"},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":149}],"metadata":{"language_info":{"mimetype":"text/x-python","name":"python","pygments_lexer":"ipython3","codemirror_mode":{"name":"ipython","version":3},"version":"3.7.4","nbconvert_exporter":"python","file_extension":".py"},"name":"2.Text_Preprocessing_with_SparkNLP_Annotators_Transformers","notebookId":21558677108572,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"colab":{"collapsed_sections":[],"name":"2.Text_Preprocessing_with_SparkNLP(Annotators_Transformers).ipynb","provenance":[],"toc_visible":true}},"nbformat":4,"nbformat_minor":0}
