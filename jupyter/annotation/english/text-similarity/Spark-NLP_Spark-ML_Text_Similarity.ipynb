{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Install java\"\n",
    "#!sudo apt-get update\n",
    "#!sudo apt-get install --fix-missing -y openjdk-8-jdk-headless -qq > /dev/null\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
    "os.environ[\"PATH\"] = os.environ[\"JAVA_HOME\"] + \"/bin:\" + os.environ[\"PATH\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "openjdk version \"1.8.0_265\"\n",
      "OpenJDK Runtime Environment (build 1.8.0_265-8u265-b01-0+deb9u1-b01)\n",
      "OpenJDK 64-Bit Server VM (build 25.265-b01, mixed mode)\n",
      "              total        used        free      shared  buff/cache   available\n",
      "Mem:             62           0          61           0           0          61\n",
      "Swap:             0           0           0\n"
     ]
    }
   ],
   "source": [
    "!java -version\n",
    "!free -g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install --ignore-installed -q pyspark==2.4.5\n",
    "#!gsutil cp gs://hadoop-lib/gcs/gcs-connector-hadoop2-latest.jar /opt/conda/lib/python3.7/site-packages/pyspark/jars/\n",
    "    \n",
    "#!pip install --ignore-installed spark-nlp==2.5.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jupyter   1823  1716  0 02:31 pts/0    00:00:00 /bin/sh -c ps -ef | grep spark\n",
      "jupyter   1825  1823  0 02:31 pts/0    00:00:00 grep spark\n"
     ]
    }
   ],
   "source": [
    "!ps -ef | grep spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sparknlp\n",
    "import json\n",
    "import os\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql import SparkSession\n",
    "from sparknlp.annotator import *\n",
    "from sparknlp.base import *\n",
    "from pyspark.sql.functions import from_unixtime\n",
    "from pyspark.sql.functions import unix_timestamp\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.functions import explode, col\n",
    "from pyspark.sql.functions import from_unixtime, to_date, asc, year, udf, explode, split, col, desc, length, rank, dense_rank, avg, sum\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "from pyspark.ml.stat import Correlation\n",
    "from pyspark.ml.clustering import BisectingKMeans\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.sql.functions import col, to_timestamp,date_format\n",
    "from pyspark import StorageLevel\n",
    "import pyspark.sql.functions as F\n",
    "from sparknlp.pretrained import PretrainedPipeline\n",
    "from collections import Counter\n",
    "from sparknlp.base import Finisher, DocumentAssembler\n",
    "from sparknlp.annotator import (Tokenizer, Normalizer,LemmatizerModel, StopWordsCleaner)\n",
    "from pyspark.ml import Pipeline\n",
    "import matplotlib.pyplot as plt\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "from sparknlp.annotator import *\n",
    "from sparknlp.common import *\n",
    "from sparknlp.base import *\n",
    "from pyspark.ml.feature import Normalizer, SQLTransformer\n",
    "from pyspark.ml.feature import BucketedRandomProjectionLSH\n",
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def start():\n",
    "    builder = SparkSession.builder \\\n",
    "        .appName(\"Spark NLP Licensed\") \\\n",
    "        .master(\"local[2]\") \\\n",
    "        .config(\"spark.driver.memory\", \"40G\") \\\n",
    "        .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
    "        .config(\"spark.kryoserializer.buffer.max\", \"2000M\") \\\n",
    "        .config(\"spark.jars.packages\", \"com.johnsnowlabs.nlp:spark-nlp_2.11:2.5.1\") \\\n",
    "        .config(\"fs.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\") \\\n",
    "        .config(\"fs.AbstractFileSystem.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS\")\n",
    "    return builder.getOrCreate()\n",
    "spark = start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jupyter   1826  1716 45 02:31 ?        00:00:13 /usr/lib/jvm/java-8-openjdk-amd64/bin/java -cp /opt/conda/lib/python3.7/site-packages/pyspark/conf:/opt/conda/lib/python3.7/site-packages/pyspark/jars/* -Xmx40G org.apache.spark.deploy.SparkSubmit --conf spark.master=local[2] --conf spark.driver.memory=40G --conf spark.kryoserializer.buffer.max=2000M --conf spark.jars.packages=com.johnsnowlabs.nlp:spark-nlp_2.11:2.5.1 --conf spark.serializer=org.apache.spark.serializer.KryoSerializer --conf spark.app.name=Spark NLP Licensed --conf fs.AbstractFileSystem.gs.impl=com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS --conf fs.gs.impl=com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem pyspark-shell\n",
      "jupyter   1988  1716  0 02:32 pts/0    00:00:00 /bin/sh -c ps -ef | grep spark\n",
      "jupyter   1990  1988  0 02:32 pts/0    00:00:00 grep spark\n"
     ]
    }
   ],
   "source": [
    "spark.version\n",
    "!ps -ef | grep spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "primaryCorpus = spark.read.option(\"header\",\"true\").csv(\"file1.csv\")\n",
    "secondaryCorpus = spark.read.option(\"header\",\"true\").csv(\"file2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert_base_cased download started this may take some time.\n",
      "Approximate size to download 389.2 MB\n",
      "[OK!]\n"
     ]
    }
   ],
   "source": [
    "documentAssembler = DocumentAssembler().setInputCol(\"text\").setOutputCol(\"document\")\n",
    "\n",
    "sentence = SentenceDetector()\\\n",
    "      .setInputCols(\"document\")\\\n",
    "      .setOutputCol(\"sentence\")\\\n",
    "      .setExplodeSentences(False)\n",
    "\n",
    "tokenizer = Tokenizer()\\\n",
    "    .setInputCols(['sentence'])\\\n",
    "    .setOutputCol('token')\n",
    "\n",
    "bertEmbeddings = BertEmbeddings\\\n",
    " .pretrained('bert_base_cased', 'en') \\\n",
    " .setInputCols([\"sentence\",'token'])\\\n",
    " .setOutputCol(\"bert\")\\\n",
    " .setCaseSensitive(False)\\\n",
    " .setPoolingLayer(0)\n",
    "\n",
    "embeddingsSentence = SentenceEmbeddings() \\\n",
    "      .setInputCols([\"sentence\", \"bert\"]) \\\n",
    "      .setOutputCol(\"sentence_embeddings\") \\\n",
    "      .setPoolingStrategy(\"AVERAGE\")\n",
    "\n",
    "embeddingsFinisher = EmbeddingsFinisher() \\\n",
    "    .setInputCols([\"sentence_embeddings\",\"bert\"]) \\\n",
    "    .setOutputCols(\"sentence_embeddings_vectors\", \"bert_vectors\") \\\n",
    "    .setOutputAsVector(True)\\\n",
    "    .setCleanAnnotations(False)\n",
    "\n",
    "\n",
    "explodeVectors = SQLTransformer() \\\n",
    ".setStatement(\"SELECT EXPLODE(sentence_embeddings_vectors) AS features, * FROM __THIS__\")\n",
    "\n",
    "vectorNormalizer = Normalizer() \\\n",
    "    .setInputCol(\"features\") \\\n",
    "    .setOutputCol(\"normFeatures\") \\\n",
    "    .setP(1.0)\n",
    "\n",
    "similartyChecker = BucketedRandomProjectionLSH(inputCol=\"features\", outputCol=\"hashes\", bucketLength=6.0,numHashTables=6)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = RecursivePipeline() \\\n",
    "      .setStages([documentAssembler,\n",
    "        sentence,\n",
    "        tokenizer,\n",
    "        bertEmbeddings,\n",
    "        embeddingsSentence,\n",
    "        embeddingsFinisher,\n",
    "        explodeVectors,\n",
    "        vectorNormalizer,\n",
    "        similartyChecker])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipelineModel = pipeline.fit(primaryCorpus)\n",
    "primaryDF = pipelineModel.transform(primaryCorpus)\n",
    "secondaryDF = pipelineModel.transform(secondaryCorpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+---+\n",
      "|                text|            features|        normFeatures|           lookupKey| id|\n",
      "+--------------------+--------------------+--------------------+--------------------+---+\n",
      "|Wall Decals Lamp ...|[-0.0029230886138...|[-1.6475124912950...|bbc5a89d7cf3354ea...|  0|\n",
      "|iphone charger ph...|[0.41062018275260...|[0.00185856293451...|37c2b6ab956f9ebd6...|  1|\n",
      "+--------------------+--------------------+--------------------+--------------------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfA = primaryDF.select(\"text\",\"features\",\"normFeatures\").withColumn(\"lookupKey\", md5(\"text\")).withColumn(\"id\",monotonically_increasing_id())\n",
    "dfA.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+---+\n",
      "|                text|            features|        normFeatures| id|\n",
      "+--------------------+--------------------+--------------------+---+\n",
      "|Curtains & Valanc...|[-0.1797953695058...|[-0.0012058615261...|  0|\n",
      "|iphone case Apple...|[-0.2585198283195...|[-8.8132988264745...|  1|\n",
      "+--------------------+--------------------+--------------------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfB = secondaryDF.select(\"text\",\"features\",\"normFeatures\").withColumn(\"id\",monotonically_increasing_id())\n",
    "dfB.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-----------------+\n",
      "|                 idA|                 idB|         distance|\n",
      "+--------------------+--------------------+-----------------+\n",
      "|iphone charger ph...|iphone case Apple...|9.144942521650227|\n",
      "|Wall Decals Lamp ...|Curtains & Valanc...|4.948642541753079|\n",
      "+--------------------+--------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#print(\"Approximately joining dfA and dfB :\")\n",
    "pipelineModel.stages[8].approxSimilarityJoin(dfA, dfB, 100, distCol=\"distance\")\\\n",
    "     .where(col(\"datasetA.id\") == col(\"datasetB.id\")) \\\n",
    "     .select(col(\"datasetA.text\").alias(\"idA\"), \\\n",
    "            col(\"datasetB.text\").alias(\"idB\"), \\\n",
    "            col(\"distance\")).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import PandasUDFType, pandas_udf\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "dfA = dfA.withColumnRenamed('text','primaryText').withColumnRenamed('features', 'primaryFeatures')\n",
    "\n",
    "dfB = dfB.withColumnRenamed('text','secondaryText').withColumnRenamed('features', 'secondaryFeatures')\n",
    "\n",
    "joinedDF = dfA.join(dfB, \"id\", \"inner\").drop(\"id\",\"normFeatures\")\n",
    "\n",
    "joinedDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "finalDF = joinedDF.toPandas()\n",
    "\n",
    "finalDF['cosine'] = finalDF.apply(lambda row: 1-cosine(row['primaryFeatures'], row['secondaryFeatures']), axis=1)\n",
    "finalDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-2-2-cpu.2-2.m48",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-2-2-cpu.2-2:m48"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
