{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "M9ps18GDtt5j"
   },
   "source": [
    "![JohnSnowLabs](https://nlp.johnsnowlabs.com/assets/images/logo.png)\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/JohnSnowLabs/spark-nlp-workshop/blob/master/jupyter/annotation/english/model-downloader/Running_Pretrained_pipelines.ipynb)\n",
    "\n",
    "## 0. Colab Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 60575,
     "status": "ok",
     "timestamp": 1589250629601,
     "user": {
      "displayName": "Christian Kasim Loan",
      "photoUrl": "",
      "userId": "14469489166467359317"
     },
     "user_tz": -120
    },
    "id": "gm0tZvJdtvgx",
    "outputId": "e8416fcf-8bed-4f34-ea7a-a0c182606908"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "openjdk version \"1.8.0_252\"\n",
      "OpenJDK Runtime Environment (build 1.8.0_252-8u252-b09-1~18.04-b09)\n",
      "OpenJDK 64-Bit Server VM (build 25.252-b09, mixed mode)\n",
      "\u001b[K     |████████████████████████████████| 215.7MB 54kB/s \n",
      "\u001b[K     |████████████████████████████████| 204kB 48.4MB/s \n",
      "\u001b[?25h  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[K     |████████████████████████████████| 122kB 8.6MB/s \n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Install java\n",
    "! apt-get update -qq\n",
    "! apt-get install -y openjdk-8-jdk-headless -qq > /dev/null\n",
    "\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
    "os.environ[\"PATH\"] = os.environ[\"JAVA_HOME\"] + \"/bin:\" + os.environ[\"PATH\"]\n",
    "! java -version\n",
    "\n",
    "# Install pyspark\n",
    "! pip install --ignore-installed pyspark==2.4.4\n",
    "\n",
    "# Install Spark NLP\n",
    "! pip install --ignore-installed spark-nlp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cyumVtb_tt5k"
   },
   "source": [
    "## Runing Pretrained models\n",
    "\n",
    "In the following example, we walk-through different use cases of some of our Pretrained models and pipelines which could be used off the shelf.\n",
    "\n",
    "There is BasicPipeline which will return tokens, normalized tokens, lemmas and part of speech tags. The AdvancedPipeline will return same as the BasicPipeline plus Stems, Spell Checked tokens and NER entities using the CRF model. All the pipelines and pre trained models are downloaded from internet at run time hence would require internet access. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Emh6GE1Ctt5l"
   },
   "source": [
    "#### 1. Call necessary imports and create the spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 60933,
     "status": "ok",
     "timestamp": 1589250629975,
     "user": {
      "displayName": "Christian Kasim Loan",
      "photoUrl": "",
      "userId": "14469489166467359317"
     },
     "user_tz": -120
    },
    "id": "DYPs5MTqtt5m",
    "outputId": "9b0c0759-b387-4a0f-83bb-c114fbded319"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.6.9 (default, Apr 18 2020, 01:56:04) \n",
      "[GCC 8.4.0]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "print(sys.version)\n",
    "\n",
    "import sparknlp\n",
    "from sparknlp.pretrained import ResourceDownloader\n",
    "from sparknlp.base import DocumentAssembler\n",
    "from sparknlp.annotator import *\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml import Pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 73758,
     "status": "ok",
     "timestamp": 1589250642818,
     "user": {
      "displayName": "Christian Kasim Loan",
      "photoUrl": "",
      "userId": "14469489166467359317"
     },
     "user_tz": -120
    },
    "id": "nfoLeCq9tt5r",
    "outputId": "e5ef3ebc-0c02-448d-f535-097ee2a90311"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark NLP version:  2.5.0\n",
      "Apache Spark version:  2.4.4\n"
     ]
    }
   ],
   "source": [
    "spark = sparknlp.start()\n",
    "\n",
    "print(\"Spark NLP version: \", sparknlp.version())\n",
    "print(\"Apache Spark version: \", spark.version)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rr6G_81ftt5v"
   },
   "source": [
    "#### 2. Create a dummy spark dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ur8mKlQTtt5v"
   },
   "outputs": [],
   "source": [
    "\n",
    "l = [\n",
    "  (1,'To be or not to be'),\n",
    "  (2,'This is it!')\n",
    "]\n",
    "\n",
    "data = spark.createDataFrame(l, ['docID','text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-TiWAq7-tt5z"
   },
   "source": [
    "#### 3. We use predefined BasicPipeline in order to annotate a dataframe with it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 106852,
     "status": "ok",
     "timestamp": 1589250675925,
     "user": {
      "displayName": "Christian Kasim Loan",
      "photoUrl": "",
      "userId": "14469489166467359317"
     },
     "user_tz": -120
    },
    "id": "OtpSOtKStt50",
    "outputId": "9c43a736-fc4d-4ea4-a58f-958016f6528f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "explain_document_ml download started this may take some time.\n",
      "Approx size to download 9.4 MB\n",
      "[OK!]\n",
      "+-----+------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|docID|              text|            document|            sentence|               token|               spell|              lemmas|               stems|                 pos|\n",
      "+-----+------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|    1|To be or not to be|[[document, 0, 17...|[[document, 0, 17...|[[token, 0, 1, To...|[[token, 0, 1, To...|[[token, 0, 1, To...|[[token, 0, 1, to...|[[pos, 0, 1, TO, ...|\n",
      "|    2|       This is it!|[[document, 0, 10...|[[document, 0, 10...|[[token, 0, 3, Th...|[[token, 0, 3, Th...|[[token, 0, 3, Th...|[[token, 0, 3, th...|[[pos, 0, 3, DT, ...|\n",
      "+-----+------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# download predefined - pipelines\n",
    "from sparknlp.pretrained import PretrainedPipeline\n",
    "\n",
    "explain_document_ml = PretrainedPipeline(\"explain_document_ml\")\n",
    "basic_data = explain_document_ml.annotate(data, 'text') \n",
    "basic_data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dT-FqWFOtt54"
   },
   "source": [
    "#### We can also annotate a single string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 748
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 106843,
     "status": "ok",
     "timestamp": 1589250675926,
     "user": {
      "displayName": "Christian Kasim Loan",
      "photoUrl": "",
      "userId": "14469489166467359317"
     },
     "user_tz": -120
    },
    "id": "TQ76lDOTtt55",
    "outputId": "46ccb9ad-94ea-4a5c-d857-3a451324cc07"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'document': ['This world is made up of good and bad things'],\n",
       " 'lemmas': ['This',\n",
       "  'world',\n",
       "  'be',\n",
       "  'make',\n",
       "  'up',\n",
       "  'of',\n",
       "  'good',\n",
       "  'and',\n",
       "  'bad',\n",
       "  'thing'],\n",
       " 'pos': ['DT', 'NN', 'VBZ', 'VBN', 'RP', 'IN', 'JJ', 'CC', 'JJ', 'NNS'],\n",
       " 'sentence': ['This world is made up of good and bad things'],\n",
       " 'spell': ['This',\n",
       "  'world',\n",
       "  'is',\n",
       "  'made',\n",
       "  'up',\n",
       "  'of',\n",
       "  'good',\n",
       "  'and',\n",
       "  'bad',\n",
       "  'things'],\n",
       " 'stems': ['thi',\n",
       "  'world',\n",
       "  'i',\n",
       "  'made',\n",
       "  'up',\n",
       "  'of',\n",
       "  'good',\n",
       "  'and',\n",
       "  'bad',\n",
       "  'thing'],\n",
       " 'token': ['This',\n",
       "  'world',\n",
       "  'is',\n",
       "  'made',\n",
       "  'up',\n",
       "  'of',\n",
       "  'good',\n",
       "  'and',\n",
       "  'bad',\n",
       "  'things']}"
      ]
     },
     "execution_count": 6,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# annotat quickly from string\n",
    "explain_document_ml.annotate(\"This world is made up of good and bad things\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6TG2d8N3tt5_"
   },
   "source": [
    "#### 4. Now we intend to use one of the fast pretrained models such as Preceptron model which is a POS model trained with ANC American Corpus "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 238
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 126761,
     "status": "ok",
     "timestamp": 1589250695851,
     "user": {
      "displayName": "Christian Kasim Loan",
      "photoUrl": "",
      "userId": "14469489166467359317"
     },
     "user_tz": -120
    },
    "id": "zSGo6qZbtt6A",
    "outputId": "f6fc49a5-ea08-461b-9460-1499fadf427c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "glove_100d download started this may take some time.\n",
      "Approximate size to download 145.3 MB\n",
      "[OK!]\n",
      "pos_anc download started this may take some time.\n",
      "Approximate size to download 4.3 MB\n",
      "[OK!]\n",
      "+-----+------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|docID|              text|            document|            sentence|               token|                 pos|     word_embeddings|\n",
      "+-----+------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|    1|To be or not to be|[[document, 0, 17...|[[document, 0, 17...|[[token, 0, 1, To...|[[pos, 0, 1, TO, ...|[[word_embeddings...|\n",
      "|    2|       This is it!|[[document, 0, 10...|[[document, 0, 10...|[[token, 0, 3, Th...|[[pos, 0, 3, DT, ...|[[word_embeddings...|\n",
      "+-----+------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "document_assembler = DocumentAssembler() \\\n",
    "    .setInputCol(\"text\")\n",
    "\n",
    "sentence_detector = SentenceDetector() \\\n",
    "    .setInputCols([\"document\"]) \\\n",
    "    .setOutputCol(\"sentence\")\n",
    "\n",
    "tokenizer = Tokenizer() \\\n",
    "    .setInputCols([\"sentence\"]) \\\n",
    "    .setOutputCol(\"token\")\n",
    "\n",
    "wordEmbeddings = WordEmbeddingsModel.pretrained().setOutputCol(\"word_embeddings\")    \n",
    "\n",
    "# download directly - models\n",
    "pos = PerceptronModel.pretrained() \\\n",
    "    .setInputCols([\"sentence\", \"token\"]) \\\n",
    "    .setOutputCol(\"pos\")\n",
    "    \n",
    "advancedPipeline = Pipeline(stages=[document_assembler, sentence_detector, tokenizer, pos, wordEmbeddings])\n",
    "\n",
    "output = advancedPipeline.fit(data).transform(data)\n",
    "output.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LPPaP1sxtt6G"
   },
   "source": [
    "#### 5. Now we proceed to download a Fast CRF Named Entity Recognitionl which is trained with Glove embeddings. Then, we retrieve the `advancedPipeline` and combine these models to use them appropriately meeting their requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 149593,
     "status": "ok",
     "timestamp": 1589250718691,
     "user": {
      "displayName": "Christian Kasim Loan",
      "photoUrl": "",
      "userId": "14469489166467359317"
     },
     "user_tz": -120
    },
    "id": "MXo_zTNatt6H",
    "outputId": "54eba9db-ffa2-4f09-f042-7339d9b23c71"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ner_crf download started this may take some time.\n",
      "Approximate size to download 10.1 MB\n",
      "[OK!]\n",
      "+-----+------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|docID|              text|            document|            sentence|               token|                 pos|     word_embeddings|                 ner|\n",
      "+-----+------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|    1|To be or not to be|[[document, 0, 17...|[[document, 0, 17...|[[token, 0, 1, To...|[[pos, 0, 1, TO, ...|[[word_embeddings...|[[named_entity, 0...|\n",
      "|    2|       This is it!|[[document, 0, 10...|[[document, 0, 10...|[[token, 0, 3, Th...|[[pos, 0, 3, DT, ...|[[word_embeddings...|[[named_entity, 0...|\n",
      "+-----+------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ner = NerCrfModel.pretrained()\n",
    "ner.setInputCols([\"pos\", \"token\", \"document\", \"word_embeddings\"]).setOutputCol(\"ner\")\n",
    "\n",
    "annotation_data = advancedPipeline.fit(data).transform(data)\n",
    "\n",
    "pos_tagged = pos.transform(annotation_data)\n",
    "ner_tagged = ner.transform(pos_tagged)\n",
    "ner_tagged.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5nuR8cxytt6L"
   },
   "source": [
    "#### 6. Finally, lets try a pre trained sentiment analysis pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 153
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 160782,
     "status": "ok",
     "timestamp": 1589250729887,
     "user": {
      "displayName": "Christian Kasim Loan",
      "photoUrl": "",
      "userId": "14469489166467359317"
     },
     "user_tz": -120
    },
    "id": "CnjUFYqctt6L",
    "outputId": "678f21ad-7db5-40b7-8708-31c644260928"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "analyze_sentiment download started this may take some time.\n",
      "Approx size to download 4.9 MB\n",
      "[OK!]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'checked': ['This', 'is', 'a', 'good', 'movie', '!!!'],\n",
       " 'document': ['This is a good movie!!!'],\n",
       " 'sentence': ['This is a good movie!!!'],\n",
       " 'sentiment': ['positive'],\n",
       " 'token': ['This', 'is', 'a', 'good', 'movie', '!!!']}"
      ]
     },
     "execution_count": 9,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PretrainedPipeline(\"analyze_sentiment\").annotate(\"This is a good movie!!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "H0sOfKV9tt6P"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "ModelDownloaderExample.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
