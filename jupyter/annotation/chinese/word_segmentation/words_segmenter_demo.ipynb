{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "words_segmenter_demo.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cpYpeEfnmWKd"
      },
      "source": [
        "![JohnSnowLabs](https://nlp.johnsnowlabs.com/assets/images/logo.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xl3k8bt-mZIc"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/JohnSnowLabs/spark-nlp-workshop/blob/master/jupyter/annotation/chinese/word_segmentation/words_segmenter_demo.ipynb)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xluzxinzKK-L"
      },
      "source": [
        "# [Word Segmenter](https://github.com/JohnSnowLabs/spark-nlp/blob/master/src/main/scala/com/johnsnowlabs/nlp/annotators/ws/WordSegmenterModel.scala)\n",
        "\n",
        "\n",
        "[WordSegmenterModel-WSM](https://en.wikipedia.org/wiki/Text_segmentation) can tokenize non-english texts. Many languages are **not whitespace seperated** and their sentences are a concationation of many symbols, like Korean, Japanese or Chinese. Withouth **understanding the language** splitting the Words into their corrosponding tokens is impossible. The WordSegmenterModel is trained to understand these languages and split then semantically correct."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MdE588BiY3z1"
      },
      "source": [
        "import os\n",
        "! apt-get update -qq > /dev/null   \n",
        "# Install java\n",
        "! apt-get install -y openjdk-8-jdk-headless -qq > /dev/null\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"PATH\"] = os.environ[\"JAVA_HOME\"] + \"/bin:\" + os.environ[\"PATH\"]\n",
        "! pip install pyspark==2.4.4 > /dev/null\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SBtn9YsW0eHz"
      },
      "source": [
        "import os\n",
        "\n",
        "# Install java\n",
        "import sparknlp\n",
        "from pyspark.ml import Pipeline\n",
        "from sparknlp.annotator import *\n",
        "from sparknlp.base import *\n",
        "\n",
        "spark = sparknlp.start()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hJFV80wXyXiQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c1c1ef34-8604-482d-d845-11ed44d48275"
      },
      "source": [
        "import pandas as pd \n",
        "document_assembler = DocumentAssembler()\\\n",
        "  .setInputCol(\"text\")\\\n",
        "  .setOutputCol(\"document\")\n",
        "\n",
        "word_segmenter = WordSegmenterModel.pretrained(\"wordseg_gsd_ud_trad\", \"zh\")\\\n",
        "        .setInputCols([\"document\"])\\\n",
        "        .setOutputCol(\"words_segmented\")    \n",
        "\n",
        "\n",
        "pipeline = Pipeline(stages=[document_assembler, word_segmenter])\n",
        "example = spark.createDataFrame(pd.DataFrame({'text': [\"\"\"然而，這樣的處理也衍生了一些問題。\"\"\"]}))\n",
        "\n",
        "\n",
        "result = pipeline.fit(example).transform(example)\n",
        "result.select('words_segmented.result').show()\n",
        "\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "wordseg_gsd_ud_trad download started this may take some time.\n",
            "Approximate size to download 1.3 MB\n",
            "[OK!]\n",
            "+----------------------------+\n",
            "|                      result|\n",
            "+----------------------------+\n",
            "|[然而, ，, 這樣, 的, 處理...|\n",
            "+----------------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "06z9uTcD1RU8"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}